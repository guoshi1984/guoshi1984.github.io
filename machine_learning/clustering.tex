\documentclass[a4paper]{article}
\usepackage{amssymb, amsmath}
\usepackage{graphicx}
\begin{document}
\section{Clustering}
\subsection{K Means} 
\noindent{\bf a. Definition}\\
Given a set of observations($x_1$, $x_2$, бн, $x_n$), where each observation is a dimensional real vector, k-means clustering aims to partition the n observations into k($\leq$n ) sets S=\{$S_1$, $S_2$,бн, $S_k$\} so as to minimize the within cluster sum of squares. Formally, the objective is to find:
\begin{align*}
argmin_{S} \sum_{i=1}^{k} \sum_{x \in S_i}|| x- u_i||^2	
= argmin_{S} \sum_{i=1}^{k} |S_i| Var S_i
\end{align*}

\noindent{\bf b.	Algorithm}\\
\noindent 1) Give the initial guess of k means $m_1$,бн,$m_k$\\
\noindent 2) Assign each observation to the cluster whose mean has the least squared Euclidean distance.\\
\noindent 3) Calculate the new means to be the centroids of the observations in the new clusters.\\
\noindent 4) $m_i^{(t+1)} = \frac{1}{S_i^{(t)} } \sum_{s_j in S_i^{(t)}} x_j$

\noindent{\bf c. Time Complexity}\\
O(nkdi), where n is the number of d dimensional vectors, k is the number of clusters and i is the number of iterations need till convergence.\\

\section{Gaussian Mixture}
{\bf a.	Idea and Definition}\\
\noindent 1)	In K means clustering, one sample point exclusively belongs to one cluster. In other words, we assign a sample point to a cluster with probability 1. In Mixture model, we assign sample point i to a cluster k with the probability $r_{ik}$, with\\ 
\begin{align*}
	\sum_{k} r_{ik} = 1
\end{align*}
The $r_{ik}$ also follows the fact\\
\begin{align*}
	\sum_{i} \sum_{k} r_{ik} = \sum_{i} 1 = N
\end{align*}
By changing the order of summation
\begin{align*}
	\sum_{i} \sum_{k} r_{ik}  = \sum_{k} \sum_{i} r_{ik}
\end{align*}
Define the weight of cluster: $w_k = \sum_{i} r_{ik} /N = \sum_{k} \omega_k * N = N$\\
So   
\begin{align*}
	\sum_{k} w_k = 1	
\end{align*}
We can also interpret $w_k$ as a prior distribution of a sample point being assigned to cluster k.\\
\noindent 2)	And for each cluster k, we define the probability of having a sample point i at $x_i$ use a normal distribution $N(x_i | u_k, \Sigma_k)$\\
Diagram:
\\
\noindent 3)	The $r_{ik} \pi_k$  and $N(x_i | u_k, \Sigma_k)$ are connected with Bayesian rule\\
\begin{align*}
	P(A|B) & = \frac{P(A) P(B|A)}{ P(B)}\\
& = \frac{P(A) P(B|A)}{ \sum_c P(C) |P(B|C) }\\
\end{align*}
According this rule, we have the following\\
\begin{align*}
& P(X_i = x_i\textrm{ and } X_i \textrm{ in cluster k})\\
= & P(X_i \textrm{ in cluster k}) P(X_i=x_i \textrm { given } X_i \textrm{ in cluster k})\\
= & P(X_i \textrm { in cluster k} |X_i = x_i) P(X_i= x_i)\\
\textrm{So }\\ 
&P(X_i \textrm{ in cluster k} |X_i = x_i)\\
&P(X_i \textrm{ in cluster k}) P(X_i=x_i \textrm{ given }X_i \textrm{ in cluster k})/(X_i= xi)
\end{align*}
Namely,\\
\begin{align*}
	r_{ik} =  \frac{\pi_k N(x_i | u_k, \Sigma_k) }{\sum_j \pi_j N(x_i | u_j, \Sigma_j)}
\end{align*}
\noindent4)	Our goal is the find $u_k$, $\Sigma_k$, $w_k$.\\

\noindent{\bf b.	Cost function and Minimization}\\
For a given point $x_i$, the likelihood function is\\
\begin{align*}
	p(x_i) = \sum_k \pi_k N(x_i | u_k, \Sigma_k)
\end{align*}
The likelihood function for the whole sample is
\begin{align*}
	\Pi_{i=1}^{N} p(x_i) = \Pi_{i=1}^{N} \sum_k \pi_k N(x_i | u_k, \Sigma_k)
\end{align*}
The goal is to minimize the negative of Log Likelihood\\ 
\begin{align*}
	L = - \sum_{i=1}^{N} ln( \sum_k \pi_k N(x_i | u_k, \Sigma_k))
\end{align*}
\noindent 1)Take the derivative with respect to $u_k$\\
\begin{align*}
	 dL/d u_k = \sum_i  \frac{\pi_k N(x_i | u_k, \Sigma_k) }{\sum_j \pi_j N(x_i | u_j, \Sigma_j) }
                       \Sigma^{-1}(x_i - u_k))
\end{align*}
We found that the term \\                         
\begin{align*}
	\frac{\pi_k N(x_i | u_k, \Sigma_k) }{\sum_j \pi_j N(x_i | u_j, \Sigma_j) }
\end{align*}                          
is exactly $r_{ik}$\\
Let the derivative equal to zero, we have
\begin{align*}
	u_k = \frac{1}{N_k} \sum_i r_ik x_i   (N_k = \sum_i r_{ik})
\end{align*}                         
\noindent 2) Taking the derivative with respect to $\Sigma_k$ gives
\begin{align*}
	\Sigma_k = 1/N_k \sum_i r_{ik} (x_i - u_k)(x_i - u_k)^T
\end{align*}
\noindent 3) Taking the derivative with respect to $\pi_k$ gives
\begin{align*}
	\pi_k = \frac{N_k}{N}
\end{align*}
We see $u_k$, $\Sigma_k$, w_k, $r_{ik}$ are mutually dependent, therefore we need to solve this iteratively.\\

\noindent{\bf c. Algorithm}\\
\noindent 1)Initialize cluster prior assignment $\pi_k = P(z_i = k)$\\
\noindent 2)Given an observation $x_i$ from cluster k,  calculate $P(x_i | z =k , u_k, \Sigma_k) = N(x_i | u_k, \Sigma_k)$\\
\noindent 3)E step\\
Given an observation $x_i$, calculate $r_{ik}$
\begin{align*}
	r_{ik} =\frac{\pi_k N(x_i | u_k, \Sigma_k) }{\sum_j \pi_j N(x_i | u_j, \Sigma_j) }
\end{align*}

\noindent 4)M step\\
\begin{align*}
	&N_k = \sum_i^N r_{ik}\\
	&\hat u_k = \sum_i^N \frac{r_{ik}}{N_k} x_i\\
	&\hat \Sigma_k = \sum_i^N \frac{ r_{ik}}{N_k} (x_i - \hat u_k) (x_i - \hat u_k)^{T}\\
	&\pi_k = \frac{N_k} {N}\\
\end{align*}
\noindent 5)Repeat 3) and 4)\\


\noindent{\bf d. Connection to K means}\\
In order to easily see how Gaussian mixture clustering relates to K means, we need to introduce another latent variable Z and consider the log likelihood function of the complete data set (X, Z).
We discussed the probability to assign a sample point to cluster k as $\pi_k$, now we denote this assignment using an indicator random variable $Z^{(i)} = Z_k = (z_{k1}, z_{k2}, бн,z_{kK})^T$\\
Where 
\begin{align*}
	z_{kj} &= 1 \textrm{ when j = k} \\
	     &= 0 \textrm{ otherwise }\\
\end{align*}
In other words, only $Z_k$ is a K dimensional vector with only kth component being 1, other components are zero. For example, $Z_1 = (1,0,0,0бн..0)^T$, $Z_k=(0,0,0,1($kth element$),бн,0)^T$\\
And 
\begin{align*}
	p(Z^{(i)} = Z_k )= \pi_k = \Pi_j \pi_j^{z_{kj}}
\end{align*}
We rewrite the likelihood function given X and Z
\begin{align*}
	L = \Pi_{i=1}^N \Pi_k^K \Pi_{j=1}^K \pi_j^{z_{kj}} N(x_i| u_j, \Sigma_j)^{z_{kj}}
\end{align*}
If we let $\pi_k = 1/K$, and the covariance matrix $= \sigma^2 {\bf I}$\\
\begin{align*}
	Log L = \sum_i \sum_k (log \pi_k -\frac{1}{2} \frac{1}{\sigma^2} ||x_i-u_k||^2)
\end{align*}
This reduces to the K means cost function\\
Now let $\sigma$ goes to 0\\
\begin{align*}
	r_{ik} = & \frac{\pi_k N(x_i | u_k, \Sigma_k )}{\sum_j \pi_j N(x_i | u_j, \Sigma_j)}\\
	       = & \frac{\pi_k exp(-\frac{(x_i -\mu_k)^2}{2\sigma^2})}{\sum_j \pi_j exp(-\frac{(x_i -\mu_j)^2}{2\sigma^2} )}\\
\end{align*}
When $\sigma$ goes to zero, the exponential term that decays the slowest survives, and the term that decays the slowest is the one that minimize $||x_i-u_k||$. Let u* be the $u_k$ that minimize $||x_i- uk||$, then 
\begin{align*}
	r_{ik} =& 1 \textrm{ for k =k*}\\
          & 0 \textrm{ otherwise}
\end{align*}
This reduces to the k means where a sample point i is solely assigned to a cluster k.\\
\end{document}