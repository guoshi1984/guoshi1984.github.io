\documentclass[a4paper]{article}
\usepackage{amssymb, amsmath}
\usepackage{graphicx}
\begin{document}
Author: Dr. Shi Guo  \hspace{30mm} Email: guoshi1984@hotmail.com\\
\line(1,0){350}\\
\section{Gradient Based Optimization Method}
\subsection{Practical use of gradient descent: Dealing with large samples}
\noindent{\bf Batch gradient descent vs. stochastic gradient descent vs. mini batch gradient descent}\\
{\bf a.	Definition}\\
\noindent 1) {\bf Batch gradient} \\
Batch gradient means using all the data point to calculate the gradient.\\
$cost = \sum_{i=1}^{N}$ -loglikelihood of ith sample\\
$grad = \frac{\partial(cost)}{\partial {\bf w}}$\\
update all parameter based on gradient\\

\noindent 2) {\bf Stochastic gradient descent}\\
The cost function used in batch gradient descent uses is the summation over all the data points. In stochastic descent the cost function we use only contains one data point, we use one data point to update parameters, iterate over all data points.\\
For m = 1 : N\\
\indent	cost = - loglikelihood of the ith sample\\
\indent  $grad = \frac{\partial(cost)}{\partial {\bf w}}$\\
update all parameter based on gradient\\

\noindent 3) {\bf Mini-Batch gradient descent} \\
We divide N samples into G = N/k groups so that each group contains k data points\\
For n = 1 : G\\
\indent cost = C $\sum_{(n-1)k} ^{nk}$ loglikelihood\\
\indent   grad = $\frac{\partial(cost)}{\partial {\bf w}}$ \\
update all parameter based on gradient\\

\noindent {\bf b.	Comparison}\\
\begin{tabular}{p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}}
&Time per iteration &	Convergence time for large data	& Sensitivity to parameters	& Smoothness\\
\hline
Batch Gradient	& Slow for large data	& Slower	& Moderate	& Smooth\\
\hline
Stochastic Gradient &	Always fast & 	Faster & High & Very noisy\\
\end{tabular}
\\

\noindent {\bf c. Practical usage}\\
Shuffle the data before running the stochastic gradient descent\\

\subsection{Adam Optimization}
(1) Weighted avarage of gradient\\
A common pratice to avoid value fluctuations during the gradient descent update is using a weighted average. For each step,
we average the gradients in the previous steps and do the update. If the cost function is $f(w)$ and its gradient is
$g(w)=\nabla f(w)$, We define at step t
\begin{align*}
	m^{(t)} = \beta m^{(t-1)} + (1 - \beta)g^{(t)} \\
	w^t = w^{(t-1)} - \alpha m^{(t)} \\
\end{align*}
Where beta is a hyperparameter. We usually choose 0.9 or 0.99. Using this weighted average, the gradient we use for update at each step is
\begin{align*}
	m^{(0)}  = 0 \\
	m^{(1)}  = \beta m^{(0)} + (1 - \beta)g^{(1)} = (1 - \beta)g^{(1)} \\
	m^{(2)}  = \beta m^{(1)} + (1 - \beta)g^{(2)} = \beta(1 - \beta)g^{(1)} + (1 - \beta)g^{(2)} \\
	m^{(3)}  = \beta m^{(2)} + (1 - \beta)g^{(3)} = \beta^2(1 - \beta)g^{(1)} + \beta(1 - \beta)g^{(2)} + (1 - \beta)g^{(3)} \\
\end{align*}
We can write above as
\begin{align*}
	m^{(t)} = (1- \beta) \sum_{i=0}^t \beta^{t-i}g^{(i)}
\end{align*}
(2) First step bias correction\\
The above weighted average causes a bias on the first step. Because the term $M^{(0)}$ is not defined and we arbitrarily set to zero. This leads to a bias on the first term, so we correct $m^{(t)}$ using $\tilde m^{(t)}$
\begin{align*}
	\tilde m^{(t)} = \frac{m_t}{1 - \beta^t} = \frac{1}{1 - \beta^t}(\beta m^{(t-1)} + (1 - \beta)g^{t})
\end{align*}
\begin{align*}
	\tilde m^{(0)} & = m^0  = 0 \\
	\tilde m^{(1)} & = \frac{m^{(1)}}{1 - \beta}= \frac{\beta}{1 - \beta} m^{(0)} + g^{(1)} = g^{(1)} \\
	\tilde m^{(2)} & = \frac{m^{(2)}}{1 - \beta^2} = \frac{\beta}{1 - \beta^2} m^{(1)} 
	+ \frac{1 - \beta}{1 - \beta^2}g^{(2)} 
		= \frac{1}{1 - \beta^2}(\beta(1 - \beta)g^{(1)} + (1 - \beta)g^{(2)}) \\
	\tilde m^{(3)} & =\frac{m^{(3)}}{1 - \beta^3} = \frac{\beta}{1 - \beta^3} m^{(2)} + \frac{(1 - \beta)}{1 - \beta^3}g^{(3)} 
	= \frac{1}{1 - \beta^3}( \beta^2(1 - \beta)g^{(1)} + \beta(1 - \beta)g^{(2)} + (1 - \beta)g^{(3)} )\\
\end{align*}
\begin{align*}
	\tilde	m^{(t)} = \frac{1 - \beta}{1 - \beta^t} \sum_{i=0}^t \beta^{t-i}g^{(i)}
\end{align*}
From above, we see that $m^{(1)} = g^{(1)}$, so gradient in the first iteration does not have any bias. Also under this correction, for any t, the sum of coefficients of $g^{(i)}$ is 1.
\begin{align*}
	\frac{1 - \beta}{1 - \beta^t} \sum_{i=0}^t \beta^{i} = \frac{1 - \beta}{1 - \beta^t}\frac{1 - \beta^t}{1 - \beta} = 1
\end{align*}
(3) Learning rate scaling\\
So far we have a constant learning rate $\alpha$. This means during one step of update, the change of $w$ is large when the gradient is large and the value of the parameter would also fluctuate. To fix this, we modify the gradient by dividing its magnitute. Similarly to $m^{(t)}$, we define
\begin{align*}
	v^{(t)} = \beta_v v^{(t-1)} + (1 - \beta_v) g^2_t\\
	\tilde v^{(t)} =  \frac{v^{(t)}}{1 - \beta_v^t}\\
	w^{(t)} = w^{(t-1)} - \alpha \frac{\tilde m^{(t)}}{\sqrt{\tilde v^{(t)}} + \epsilon} \\
\end{align*}
Where the $\epsilon$ is a small positive number in order to prevent dividing by zero.
(4) Summary 
\begin{align*}
	g^{(t)} = \nabla_w f(w)\\
 	m^{(t)} = \beta m^{(t-1)} + (1 - \beta)g^{(t)} \\
	v^{(t)} = \beta_v v^{(t-1)} + (1 - \beta_v) g^2_t\\
	 \tilde m^{(t)} = \frac{m_t}{1 - \beta^t} 
	 \tilde v^{(t)} =  \frac{v^{(t)}}{1 - \beta_v^t}\\
	 w^{(t)} = w^{(t-1)} - \alpha \frac{\tilde m^{(t)}}{\sqrt{\tilde v^{(t)}} + \epsilon} \\
\end{align*}
\subsection{Conjugate Gradient Method}
{\bf a. The idea of A orthogonality}\\
There exists tremendous materials online explaining conjugate gradient method. However, after reading many versions of explanations, I am still confused that why letting moving directions conjugate to each other eventually leads to a solution. So I will be explaining the intuition first such that the idea of CONJUGATE comes more natural to understand.\\
\noindent 1){\bf The quadratic form and its gradient.} Let us start with a quadratic function 
\begin{align*}
f({\bf x})   =  \frac{1}{2}{\bf x}^T A {\bf x}
\end{align*}
Let
\begin{align*}
A =  \left( \begin{array} {c c}
			2 & 0 \\
			0 & 1 \\
			\end{array}
			\right)
\end{align*}
and 
\begin{align*}
{\bf x} = (x_1, x_2 ) ^T
\end{align*}
Then
\begin{align*}
	f(x_1, x_2) 
		   & = \frac{1}{2} 
		   \left( \begin{array} {c c}
			x_1 & x_2 \\
			\end{array}
			\right)
		   \left( \begin{array} {c c}
			2 & 0 \\
			0 & 1 \\
			\end{array}
			\right)
			\left( \begin{array} {c}
			x_1  \\
			x_2  \\
			\end{array}
			\right) \\
& = \frac{1}{2} ( 2x_1^2 + x_2^2)
\end{align*}
The gradient of $f({\bf x})$ is\\
\begin{align*}
   \nabla f(x_1, x_2)  & = (\frac{\partial }{\partial x_1} f(x_1,x_2), 
	\frac{\partial }{\partial x_2} f(x_1, x_2) ) \\
                              & = (2x_1 , x_2) \\
        & =  \left( \begin{array} {c c}
			2 & 0 \\
			0 & 1 \\
			\end{array}
			\right)
\left( \begin{array} {c}
			x_1  \\
			x_2  \\
			\end{array}
			\right)\\
            & = A{\bf x}
\end{align*}
\noindent 2){\bf Minimization using gradient descent.} Suppose we arbitrarily choose a starting point $(x^{(0)}, y^{(0)}) = (1, 1)$. Based on the principle of gradient descent, we move point 0 to point 1 along the opposite direction of the gradient. Namely, the direction we move should be
\begin{align*}
	{\bf d} =  A{\bf x} = (-2x_1, -x_2)^T = (-2, -1)^T
\end{align*}
After finding the direction, we need to determine the step $\alpha$ that we need to move, our next point is
\begin{align*}
	  (x_1^{(1)}, x_2^{(1)})^T & = (x_1^{(0)}, x_2^{(0)})^T + \alpha {\bf d}^{(0)} \\
                                              & = (x_1^{(0)} , x_2^{(0)})^T + \alpha(-2, -1)^T \\
	  					 & = (1 - 2\alpha, 1 -\alpha)\\
	f(x_1^{(1)}, x_2^{(1)}) & =  2(1 - 2\alpha)^2 + (1 -\alpha)^2\\
                                             & = 9\alpha ^2 -  10 \alpha + 3\\
\end{align*}
We choose $\alpha$ such that $\alpha$ minimizes $f(x_1^{(1)}, x_2^{(1)})$ and we achieve this by setting
$\frac{\partial f }{\partial \alpha} =0 $.
\begin{align*}
\frac{\partial f }{\partial \alpha} = 18\alpha-10 = 0
\end{align*}
 We get $\alpha = \frac{5}{9}$, and the corresponding $(x_1^{(1)}, x_2^{(1)} ) $ is\\
\begin{align*}
	 (x_1^{(1)}, x_2^{(1)}) & = (x_1^{(0)}, x_2^{(0)}) + \alpha {\bf \hat d}^{(0)} \\
	 x_1^{(1)} & = 1 + \frac{5}{9} \times (-2) = -\frac{1}{9} \\ 
	 x_2^{(1)} & = 1 - \frac{5}{9} = \frac{4}{9}\\
\end{align*}
{\bf A-orthogonality} Since we can easily know the minimum point of $f(x_1, x_2)$ is (0,0) without doing any calculation. We can calculate the error term which gives us how far we are still off the minimum point. We define the error term as
\begin{align*}
	e^{(1)} = (0, 0) - (x_1^{(1)}, x_2^{(1)}) = (\frac{1}{9}, -\frac{4}{9}).
\end{align*}
Last, let us work on an interest fact by look at a matrix multiplication
\begin{align*}
	&{\bf d^{(0)}} A {\bf e^{(1)}} \\
	& = \left( 
	\begin{array}{c c}
	-2 & -1\\
	\end{array}
	\right)
	\left( 
	\begin{array}{c c}
	2 & 0 \\
	0 & 1 \\
	\end{array}
	\right)
	\left( 
	\begin{array}{c}
	\frac{1}{9} \\
	-\frac{4}{9} \\
	\end{array}
	\right)
	=0
\end{align*}
	Orthogonal!!! This means the error term and moving direction are A-orthogonal. The fact of orthogonality holds if the $f({\bf x})$ has more than two variables. This is a very interesting point. But why?\\
{\bf b. Proof of A-orthogonality}\\
Now we suppose the $f({\bf x}) = f(x_1, x_2, ... , x_i, ..., x_N )$ take a more general quadratic form\\
\begin{align*}
	f({\bf x}) & =	\frac{1}{2} {\bf x}^T A {\bf x} - {\bf b}^T {\bf x} + c \\
			   & =	\frac{1}{2} ( \sum_i a_{ii} x_i^2 + 2\sum_{i, j, i < j}  a_{ij} x_i x_j ) - \sum_i b_i x_i + c \\     
\end{align*}
Now taking the derivative,
\begin{align*}
	& \frac{\partial f({\bf x} )}{\partial x_k} \\
  = & a_{kk} x_k + \sum_{j \neq k} a_{kj} x_j  - b_k\\
  = & \sum_j a_{kj} x_j - b_k\\ 	
\end{align*}
The last line is the same as the first row result of matrix Ax -b. So 
\begin{align*}
	& \left( 
	\begin{array}{c}
	\frac{\partial f({\bf x})}{\partial x_1} \\
	\frac{\partial f({\bf x})}{\partial x_2} \\
	...\\
	\frac{\partial f({\bf x})}{\partial x_N} \\
	\end{array}
	\right) \\
	 = & \left( 
	\begin{array}{c}
     \sum_j a_{1j} x_j - b_1\\ 	
     \sum_j a_{2j} x_j - b_2\\ 	
     ...\\ 	
     \sum_j a_{Nj} x_j - b_N\\ 	
	\end{array}
	\right) \\
	 = & Ax-b
\end{align*}
We choose $f({\bf x}^{(1) })$ such that
\begin{align*}
& \frac{\partial f(x^{(1)}) }{\partial \alpha}  = 0 \\
\end{align*}
So,
\begin{align*}
& \frac{\partial f(x^{(1)}) }{\partial \alpha} \\
& = \sum_i\frac{\partial f(x_i^{(1)} )}{\partial x_i^{(1)}}\frac{\partial x_i^{(1)} }{\partial \alpha}
& = \nabla f({\bf x}^{(1)}) \cdot d^{0} 
& = (A x^{(1)} - b) \cdot d^{0}
\end{align*}
While we know that
\begin{align*}
	Ae^{(1)} = A(x^{(min)} - x^{(1)})  = b-A x^{(1)}
\end{align*}
This leads to
\begin{align*}
	{d^{(0)}}^T A e^{(1)} = 0
\end{align*}
So by choosing $\alpha$ that minimize $f({\bf x^{(1)}})$, we automatically guarantee that the moving direction is A-orthogonal to the error term. Proof completed. 
%We note the in the proof above we use the moving direction at step 0 and the error term in step 1, but it is easy to generalize this relationship to any step. In other words, the following relationship holds at step i.
%\begin{align*}
%	{d^{(i)}}^T A e^{(i+1)} = 0
%\end{align*}
%The orthogonality relationship is due to the nature that $f({\bf x})$is quadratic. If we take a f(x) with a different form, it is not guaranteed to have this relationship. \\
%The A-orthogonal relationship provides us a way to solve for $\alpha$. We notice that $e^{(1)} = e^{(0)} + \alpha d^{(0)}$, then we have
%\begin{align*}
%	{d^{(0)}}^T A (e^{(0)} + \alpha d^{(0)}) = 0
%\end{align*}
%Solve for $\alpha$\\
%\begin{align*}
%	\alpha & = -\frac{{d^{(0)}}^T A e^{(0)} }{{d^{(0)}}^T A d^{(0)} }\\
%	       & = \frac{{d^{(0)}}^T A  (x_{min} - x^{(0)}) }{{d^{(0)}}^T A d^{(0)} }\\
%	       & = \frac{{d^{(0)}}^T b  - {d^{(0)}}^T A  x^{(0)} }{{d^{(0)}}^T A d^{(0)} }%\\
%\end{align*}

\noindent{\bf c. Steps of convergence}\\
In the example we did in the intuition, the $f({\bf x})$ only has two variables. And we have proved that after moving one step, the error term is A-orthogonal to the moving direction. So if we can enforce our next moving direction is A-orthogonal to the current one, how many steps does the process converge?  The answer is by enforcing each moving directions are A-orthogonal to each other, the algorithm can converge using exact n step, where n is the number of variables.\\
\noindent Proof:\\
We express the error term $e^{(0)}$ at step 0 as a linear combinations of n searching directions\\
\begin{align*}
	e^{(0)} = \sum_{j=0} ^{n-1} \delta_j d^{(j)}\\
\end{align*}
Where $\delta_j$ is a scalar. We find $\delta_j$ by multiplying ${d^{(k)}}^T A$ on both sides
\begin{align*}
	{d^{(k)}}^T A e^{(0)} & = \sum_{j=0} ^{n-1} \delta_j {d^{(k)}}^T A d^{(j)} \\
	{d^{(k)}}^T A e^{(0)} & = \delta_k {d^{(k)}}^T A d^{(k)} \\
\end{align*}

\begin{align*}
	\delta_k  = & \frac{{d^{(k)}}^T A e^{(0)}}{{d^{(k)}}^T A d^{(k)}} \\
			  = & \frac{{d^{(k)}}^T ( A e^{(k)} + \sum_{i=0}^{k-1} \alpha_i d^{(i)})}{{d^{(k)}}^T A d^{(k)}} \\
                             & ({d^{(k)}}^T d^{(i)} = 0 ) \\
			 = & \frac{{d^{(k)}}^T A e^{(k)}}{{d^{(k)}}^T A d^{(k)}}
\end{align*}
If we let
\begin{align*}
	\alpha_i = \delta_i
\end{align*}
So
\begin{align*}
	e^{(k)} = e^{(0)} - \sum_{i=0}^{k-1} \alpha_i d^{(i)} \\
	        = \sum_{i=0}^{n-1} \delta_i d^{(i)} - \sum_{i=0}^{k-1} \alpha_i d^{(i)}
	        = \sum_{j=k}^{n-1} \delta_i d^{(i)}
\end{align*}
We see that when i = n, $e^{(n)} = 0$. So we reach the convergence after exact n steps.\\
{\bf d. algorithm}\\
For minimizing
\begin{align*}
f(\bf x) = {\bf x}^T A {\bf x} - 2b^T {\bf x}
\end{align*}
\begin{align*}
x^{(i)} & =  x^{(i-1)} + \alpha^{(i-1)}  d^{(i-1)}\\
r^{(i)} & =   r^{(i-1)} - \alpha^{(i-1)} A  d^{(i-1)}\\
d^{(i)} & =  r^{(i)} + \beta^{(i-1)} d^{(i-1)}\\
\end{align*}
at each step
\begin{align*}
\alpha^{(i)} & = \frac{{r^{(i)}}^T r^{(i)}}{ {d^{(i)}}^T A  d^{(i)}}\\
\beta^i & = \frac{{ r^{(i+1)}}^T r^{(i+1)}}{{r^{(i)}}^T r^{(i)}}\\
\end{align*}
\section{Hession Based Method}
\subsection{Newton Method}
\noindent {\bf a. Newton Method Principles}\\
Based on Taylor's expansion if we are at $x_0$, we try to find $\delta x$ so that $x_0 + \delta x$ is closer to the stationary point.\\
\begin{align*}
	f(x_0 + \delta x ) = f(x_0) + f^{'}(x_0) \delta x + f^{''}(x_0) (\delta x) ^ 2
\end{align*}
take the derivative
\begin{align*}
	d f(x_0 + \delta x)/d x = f^{'}(x_0)  + f^{''}(x_0) \delta x
\end{align*}
therefore
\begin{align*}
	\delta x = - \frac{f^{'}(x_0)}{f^{''} (x_0)} \\
	X^{(t+1)} = X^{t}  - \frac{f^{'}(x_0)}{f^{''} (x_0)} \\
\end{align*}

\noindent{\bf b. Matrix Forms}
\begin{align*}
	x^{(t+1)} = x^{t}  - H^{-1}(f(x^{t})) \nabla f(x^{t})
\end{align*}
 where H is the Hessian matrix.\\
 
\noindent{\bf c. Connection with Gradient descent}
The newton method can be reduce to gradient descent method by taking Hessian matrix as Identity matrix\\

\noindent{\bf d. Pros} 
Since it utilizes the second order derivative, it converges much faster than gradient descent.\\
For quadratic function, the equation from the Taylor expansion is exact, therefore the stationary point can be found using only one step.\\

\noindent{\bf e. Cons}
Need to evaluate the inverse of the Hessian Matrix, so it is computationally expensive.\\

\subsection{Quasi Newton}
Newton method requires the inverse of the Hessian matrix, which is usually not easy to solve. So we need to find an approximation of the Hessian. Similar to the way we solve for gradient, we can use finite difference method, in which the gradient is
\begin{align*}
	grad f(x) = \frac{f(x+\delta x) - f(x)} {\delta x}, 
\end{align*}
This is only exact when $\delta x$ approaches zero.  For 2nd order derivative, we can write\\
\begin{align*}
	f^{'}(x) = \frac{f^{'}(x+\delta)- f^{'}(x)}{\delta}
\end{align*}
Again this is only exact when $\delta$ is zero. Based on this idea we
replace the Hessian Matrix with an approximation that satisfies the following approximation\\
\begin{align*}
	\nabla f(x + \delta x) = \nabla f(x) + B\delta x
\end{align*}
This is quasi newton method. Various Quasi Newton methods exist with different choice of B.\\

\section{Levenburg Marquadt}
This Method adds a scaled Identity matrix uI to the Hessian, for large u and small Hessian, the method is equivalent to gradient descent with step size 1/u.\\
\end{document}
