\documentclass[a4paper]{article}
\usepackage{amssymb, amsmath}
\usepackage{graphicx}
\begin{document}
Author: Dr. Shi Guo  \hspace{30mm} Email: guoshi1984@hotmail.com\\
\line(1,0){350}
\section{Jacobi Method for Solving Eigenvalues}\\
\subsection{Intuition}\\
Imagine we have a simple diagonal matrix C\\
\begin{align*}
\left( \begin{array}{cc}
1 & 0\\
0 & -1\\
\end{array} \right)
\end{align*}
Finding the eigenvalue and eigenvector is trival. Its eigenvalue is $\lambda_1 = 1$, $\lambda_2 = -1$. The eigenvectors are
\begin{align*}
	v_1 = 
	\left(  \begin{array} {c}
		1 \\
		0 \\
	\end{array} \right) 
\end{align*}
and
\begin{align*}
	v_2 = 
	\left( \begin{array} {c}
		0 \\
		1 \\
	\end{array} \right) 
\end{align*}
We now consider a rotation matrix that rotates the eigenvectors by 45 degree angle
\begin{align*}
	u_1 & = R v_1 \\
	& =\left(  \begin{array} {cc}
		\frac{\sqrt{2}}{2}& -\frac{\sqrt{2}}{2}\\
		\frac{\sqrt{2}}{2}& \frac{\sqrt{2}}{2}\\
	\end{array} \right) 
	\left(  \begin{array} {c}
		1 \\
		0 \\
	\end{array} \right) 
	=
	\left(  \begin{array} {c}
		\frac{\sqrt{2}}{2} \\
		\frac{\sqrt{2}}{2} \\
	\end{array} \right) 
\end{align*}
The eigenequation still holds for $u_1$ because
\begin{align*}
	C v_1 = C R^{-1} R v_1 = \lambda v_1 \\
	R C R^{-1} R v_1 = \lambda R v_1 \\
	R C R^{-1} u_1 = \lambda  u_1 \\
\end{align*}
Let $R C R^{-1} = A $, where A is
\begin{align*}
	\left( \begin{array}{cc}	
		0 & 1\\
		1 & 0\\
\end{array} \right)
\end{align*}
Up to now we see we can transform a diagonal matrix to non-diagonal, still symmetric matrix by doing rotation. \\
\subsection{Jacobi Method}
The Jacobi method reverse the idea above by rotating a non-diagonal matrix back to a diagonal matrix. 
\begin{align*}
	A = R C R^{-1} \\
	C = R^{-1} A R \\
\end{align*}
where matrix C is diagonal.\\
\subsection{Eligibility}
Since we apply similar transformation by rotation matrix and eventually we can the diagonal matrix which is symmetric, the original matrix has to be symmetrical.\\
\subsection{Algorithm}
The Jacobi iteration for a matrix A is
\begin{align*}
	A^{(k)} = R^T^{(k)}_{pq}(\theta) A^{k-1} R_{pq}^{(k)} (\theta)
\end{align*}
Where
\begin{align*}
	R_{pq}(\theta)
	= 
	\left( \begin{array}{ccccc}
I      & 	0 	& 	0 	 & 	 0 		& 	0 	\\
0	   & 	cos(\theta) 	& 	0 	 &	 sin(\theta) 		& 	0	\\
0 	   &    0   &   I  	 &   0		&	0	\\
0 	   &    -sin(\theta)   &   0    &   cos(\theta)		& 	0	\\
0      &    0   &   0    & 	 0   	& 	I	\\\end{array} \right)
\end{align*}
It is an Identity matrix replaced by an rotation matrix on pth and qth columns and rows. The iteration is chosen to reduce the sum of the squares of the off-diagonal elements, which for any square matrix A is
\begin{align*}
	||A||_F^2 - \sum_i a_{ii}^2
\end{align*}
The orthogonal similarity transforms preserve the Frobenius norm
\begin{align*}
	|| A^{(k)}||_F = ||A^{(k-1)}||_F
\end{align*}
Because the rotation matrix change only (p,p), (q,q), (p,q), (q,p) positions. We have
\begin{align*}
	(a_{pp}^{(k)})^2 + (a_{qq}^{(k)})^2+ 2 (a_{pq}^{(k)})^2 = (a_{pp}^{(k-1)})^2 + (a_{qq}^{(k-1)})^2 + 2 (a_{pq}^{(k-1)})^2   
\end{align*}
The off-diagonal sum of squares at the kth stage in terms of that at k-1 th stage is
\begin{align*}
	& || A^{(k)}||^2_F - \sum_i (a_{ii}^{(k)}) \\
	= & || A^{(k)}||^2_F - \sum_{i \neq p,q} (a_{ii}^{(k)}) -( (a_{pp}^{(k)})^2 + (a_{qq}^{(k)})^2 ) \\
	= & || A^{(k)}||^2_F - \sum_i (a_{ii}^{(k-1)}) - 2 (a_{pq}^{(k-1)})^2  + 2 (a_{pq}^{(k)})^2  \\
\end{align*}
In order to minimize this, we need
\begin{align*}
	& a_{pq}^{(k)} = 0\\
	& a_{pq}^{(k-1)} = max _{i<j} |a_{ij} ^ {(k-1)}|\\
\end{align*}
This implies
\begin{align*}
	a_{pq}^{(k-1)} (cos^2 \theta - sin^2 \theta) + (a_{pp}^{k-1} - a_{qq}^{k-1}) cos \theta sin \theta = 0
\end{align*}
Solve for $\theta$
\begin{align*}
	tan(2\theta) & = \frac{2 a_{pq}^{(k-1)}}{a_{pp}^{k-1} - a_{qq}^{k-1}} \\
	tan(\theta) & = \frac{tan(2 \theta)}{1 + \sqrt{1 + tan^2 (2 \theta)}} \\
	cos \theta & = \frac{1}{\sqrt{1 + tan^2 \theta}} \\
	sin \theta & = cos \theta tan \theta
\end{align*}
We use the above formula to update the matrix(by rotation). \\
\subsection{Eigenvectors}
Since 
\begin{align*}
	A = R C R^{-1}
	C = R^{-1} A R 
\end{align*}
We know the eigenvectors for diagonal matrix is unit vector e, so
\begin{align*}
	Ce = R^{-1} A R e = \lambda e
\end{align*}
Multiply by R from the left, 
\begin{align*}
	RPe = \lambda Re
\end{align*}
so $Re$ is the eigenvector for A.
\section{Singular Value Decomposition(SVD)}
\begin{align*}
	A = U \Sigma V^T
\end{align*}
\subsection{SVD using Jacobi}
{\bf a. Definition}\\
A matrix $A(m \times n)$ can be decompose into three matrix
\begin{align*}
	A = U \Sigma V^T
\end{align*}
where $U$ is $m \times m$ matrix, $\Sigma$ is $m \times n$ matrix with diagonal elements only, and $V$ is $n \times n$ matrix.
{\bf Jacobi method}
SVD using Jacobi is based on the following fact
\begin{align*}
	& A = U \Sigma V^T \\
	& A^T A = (U \Sigma V^T)^T U \Sigma V^T = V \Sigma^T U^T U\Sigma V^T \\
	& A^T A V = V \Sigma^T \Sigma V^T V \\
	& A^T A V = V \Sigma^T \Sigma = \Sigma^T \Sigma V
\end{align*}
So V is the eigenvector matrix of $A^T A$, and $\Sigma^T \Sigma$ is a diagonal matrix whose elements are $\sigma_i^2$, where $\sigma_i$ is the eigenvalue of A. This can be shown as the following.\\
If $m >n $, then\\
\begin{align*}
	\Sigma = \left(  \begin{array} {cc}
		\sigma_1 & 0 \\
		0 & \sigma_2\\
		0 & 0
	\end{array} \right)
\end{align*}

\begin{align*}
	\Sigma^T \Sigma = \left(  \begin{array} {ccc}
		\sigma_1 & 0 & 0 \\
		0 & \sigma_2 & 0\\
	\end{array} \right)
	\left(  \begin{array} {cc}
		\sigma_1 & 0 \\
		0 & \sigma_2\\
		0 & 0
	\end{array} \right)
	= \left(  \begin{array} {cc}
		\sigma_1^2 & 0 \\
		0 & \sigma_2^2\\
	\end{array} \right)
\end{align*}	

If $m < n $, then\\
\begin{align*}
	\Sigma = \left(  \begin{array} {ccc }
		\sigma_1 & 0 & 0\\
		0 & \sigma_2 & 0\\
	\end{array} \right)
\end{align*}

\begin{align*}
	\Sigma^T \Sigma = \left(  \begin{array} {cc}
		\sigma_1 & 0  \\
		0 & \sigma_2 \\
		0 & 0\\
	\end{array} \right)
	\left(  \begin{array} {ccc}
		\sigma_1 & 0 & 0\\
		0 & \sigma_2 & 0\\
	\end{array} \right)
	= \left(  \begin{array} {ccc}
		\sigma_1^2 & 0 & 0\\
		0 & \sigma_2^2 & 0\\
		0 & 0 & 0\\
	\end{array} \right)
\end{align*}
So $\sigma_i$ can be calculated by taking the square root of the first r = min(m,n) largest values of $\sigma_i^2$.\\
The U can be obtains in two ways. One is using Jacobi method again.
\begin{align*}
	& A A^T =  U \Sigma V^T (U \Sigma V^T)^T = U \Sigma V^T V \Sigma U^T = U \Sigma \Sigma^T U^T \\
	& A A^T U =  U \Sigma \Sigma^T U^T U \\
	& A A^T U =  \Sigma \Sigma^T U \\
\end{align*}
So U is the eigenvector matrix of $A A^T$, but this requires additional Jacobi diagonalization. An alternative way is to consider\\
\begin{align*}
	AV = U \Sigma V^T V = U \Sigma\\
\end{align*}
If $m > n $, then\\
\begin{align*}
	U\Sigma = \left(  \begin{array} {cc}
		u_{11}\sigma_1 & u_{12}\sigma_2 \\
		u_{21}\sigma_1 & u_{22}\sigma_2 \\
		u_{31}\sigma_1 & u_{32}\sigma_2 \\
	\end{array} \right)
\end{align*}	
If $m < n $, then\\
\begin{align*}
	U\Sigma = \left(  \begin{array} {ccc}
		u_{11}\sigma_1 & u_{12}\sigma_2 & 0\\
		u_{21}\sigma_1 & u_{22}\sigma_2 & 0\\
	\end{array} \right)
\end{align*}
So if we take $U\Sigma$ matrix, divided by the ith column with $\sigma_i$, we can obtain U matrix.\\
\end{document}