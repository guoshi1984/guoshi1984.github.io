\documentclass[a4paper]{article}
\usepackage{amssymb, amsmath}
\usepackage{graphicx}
\begin{document}
\section{Linear Regression}
\subsection{Linear regression and Least Square Solution}
\begin{align*}
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{\epsilon}
\end{align*}
where:
\begin{itemize}
    \item $\mathbf{y}$ is $n\times 1$ vector of outcomes
    \item $\mathbf{X}$ is $n\times k$ matrix of regressors (full column rank)
    \item $\boldsymbol{\beta}$ is $k\times 1$ parameter vector
    \item $\mathbf{\epsilon}$ is $n\times 1$ error vector
\end{itemize}
{\bf Assumptions}\\
1. Linear \\
2. X matrix has full rank. In other words, no multicollinearity. \\
2. error term has zero mean $\mathbb{E}[\mathbf{\epsilon}|X] = 0$\\
3. Homescedasticity or equal variance of $\epsilon$. In other words, no autocorrelation between disturbances.$cov(\epsilon_i, \epsilon_j)=0$.\\
6. Number of obsearvations n must be greater than the number of parameters.\\
{\bf Least Square Solution}\\
The cost function is given by
\begin{align*}
f(\boldsymbol{\beta}) = ||\mathbf{y}-\mathbf{X}\boldsymbol{\beta}||^2 & = (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^T(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})
                      & = \mathbf{y}^T \mathbf{y} - \mathbf{y}^T\mathbf{X} \boldsymbol{\beta} - \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{Y} +\boldsymbol{\beta}^T\mathbf{X}^T \mathbf{X} \boldsymbol{\beta}
\end{align*}
Since third term are scalar, 
\begin{align*}
 \beta^TX^TY = (\beta^TX^TY)^T = Y^TX\beta 
\end{align*}
\begin{align*}
f(\beta) = Y^T Y - 2Y^TX\beta - \beta^TX^T X \beta
            = Y^T Y - 2(X^TY)^T\beta + \beta^TX^T X \beta
\end{align*}
The first term is a constant and its derivative is zero. \\
{\bf The deriviative of 2nd term}\\
Consider the derivative of $\alpha^T \beta$ with respect to $\beta$.
\begin{align*}
\boldsymbol{\alpha}^T \boldsymbol{\beta} = \Sigma \alpha_i \beta_i \\
\frac{\partial{\boldsymbol{\alpha}^T\boldsymbol{\beta}}}{\partial \beta_i} = \alpha_i\\
\end{align*}
Write the derivative in matrix form
\begin{align*}
\left(  \begin{array} {c}
		\frac{\partial{\boldsymbol{\alpha}^T\boldsymbol{\beta}}}{\partial \beta_1}\\
		\frac{\partial{\boldsymbol{\alpha}^T \boldsymbol{\beta}}}{\partial \beta_2}\\
                  ...\\
		\frac{\partial{\boldsymbol{\alpha}^T \boldsymbol{\beta}}}{\partial \beta_3}\\
		\end{array}
		\right) 
=\left(  \begin{array} {c}
		\alpha_1\\
		\alpha_2\\
                  ...\\
		\alpha_p\\
		\end{array}
		\right) 
\end{align*}
So if we let $\alpha= X^TY$, we have
\begin{align*}
\frac{\partial{2(X^TY)^T\beta}}{\partial \beta} = 2X^TY
\end{align*}
{\bf The derivative of 3rd term}\\
let $A=X^TX$,
\begin{align*}
\beta^TX^T X \beta = \beta^T
\left(  \begin{array} {c}
		\Sigma_i A_{1k}\beta_{k}\\
		 \Sigma_i A_{2k}\beta_{k}\\
                  ...\\
		 \Sigma_k A_{pk}\beta_{k}\\
		\end{array}
		\right) 
 =  \Sigma_j \beta_j (\Sigma_k A_{jk}\beta_k)
\end{align*}
To calculate the derivative of $f(\beta)$, we note there are only 3 cases that the derivative does not vanish\\
1) l = j = k 
\begin{align*}
\frac{f(\boldsymbol \beta)}{\partial \beta_l} 
= 2 A_{ll} \beta_l 
\end{align*}
2) l=j, j $\neq$ k
\begin{align*}
\frac{f(\boldsymbol \beta)}{\partial \beta_l} 
= \Sigma_{k, k\neq l} A_{lk}\beta_{k}\\
\end{align*}
3) l=k, j $\neq$ k
\begin{align*}
\frac{f(\boldsymbol \beta)}{\partial \beta_l} 
= \Sigma_{j, j\neq l} A_{jl}\beta_{j} = \Sigma_{j,  j\neq l} A^T_{lj}\beta{j}
\end{align*}
Therefore
\begin{align*}
\frac{f(\boldsymbol \beta)}{\partial \beta_l}  = A_{ll} \beta_l + \Sigma_{k, k\neq l} A_{lk}\beta_{k} + A_{ll} \beta_l + \Sigma_{j, j \neq l} A^T_{lj}\beta{j} \\
= \Sigma_{k} A_{lk}\beta_{k} + \Sigma_{j} A^T_{lj}\beta{j}
\end{align*}
The first term is the lth row of vector $A\beta = X^TX\beta$, and the 2nd term is the lth row of vector$A^T\beta=X^TX\beta$. So we put the whole derivative in matrix form
\begin{align*}
\frac{f(\boldsymbol \beta)}{\partial {\boldsymbol \beta}} = -2X^TY+2X^TX\beta
\end{align*}
which is a px1 vector with each row corresponding to the derivative with respect to $\beta_i$
letting the derivative equal to zero yields the {\bf normal equation} and the estimation of $\beta$\\
Normal equation
\begin{align*}
(X^TX) \hat \beta = X^TY
\end{align*}
Estimator of $\beta$
\begin{align*}
\hat \beta = (X^TX)^{-1}X^TY
\end{align*}
Using the rule of matrix mutiplication, we can rewrite $X^TX$ as the f
\begin{align*}
X^TX & =
 \left( \begin{array} { c  c  c c } 
                   \sum_{i=1}^N x_{1i}^Tx_{i1} &  \sum_{i=1}^Nx_{1i}^Tx_{i2} & ... & \sum_{i=1}^N x^T_{1i}x_{ik}   \\
                   ... & ... & ... & \\
                   \sum_{i=1}^N x_{ki}^Tx_{i1} &  \sum_{i=1}^Nx_{ki}^Tx_{k2} & ... & \sum_{i=1}^Nx^T_{ki}x_{ik}   \\
           \end{array} \right) \\
    & =  \sum_{i=1}^N
 \left( \begin{array} { c  c  c c } 
                   x_{i1}x_{i1} & x_{i1}x_{i2} & ... & x_{i1}x_{ik}   \\
                   ... & ... & ... & \\
                   x_{ik}x_{i1} & x_{ik}x_{k2} & ... & x_{ik}x_{ik}   \\
           \end{array} \right) \\
      &  =  \sum_{i=1}^N
 \left( \begin{array} { c  } 
                   x_{i1}    \\
                   ...  \\
                   x_{ik}    \\
           \end{array} \right)
 \left( \begin{array} { c c c c } 
                   x_{i1}  & ... &  ... x_{ik}\\ 
           \end{array} \right) \\
     & = \sum_{i=1}^N X_i X_i^T
\end{align*}
where $X_i$ is $K \times 1$ vector, and $X_i X_i^T$ is $K \times K $ matrix.
Similarly,
\begin{align*}
X^TY & = \sum_{i=1}^N X_i y_i
\end{align*}
where $X_i$ is $K \times 1$ vector, $y_i$ is a scalor, and $X_iy_i$ is $K \times 1$ vector.
So the estimator of $\beta$ is
\begin{align*}
\hat \beta = (\sum_{i=1}^N X_i X_i^T)^{-1} (\sum_{i=1}^N X_i y_i)
\end{align*}
{\bf Least Square Estimator for Simple Linear Regression}\\
\begin{align*}
y =  \beta_0 + \beta_1 X + \epsilon\\
\end{align*}
\begin{align*}
	&\left(  \begin{array} {c}
		\beta_{0} \\
		\beta_{1} \\
		\end{array}
		\right) \\
=& (X^TX)^{-1}X^TY \\
=&\left( \left(  \begin{array} {cccc}
		1 & 1 &... &1\\
		x_1 & x_2 &... &x_n \\
		\end{array}
		\right) 
    \left(  \begin{array} {cc}
		1 & x_1 \\
		1 & x_2 \\
                   1 & x_n \\
		\end{array}
		\right) 
\right)^{-1}
\left(  \begin{array} {cccc}
		1 & 1 &... &1\\
		x_1 & x_2 &... &x_n \\
		\end{array}
		\right) 
\left(  \begin{array} {c}
		y_1 \\
		y_2 \\
                  ... \\
                  y_n
		\end{array}
		\right) \\
=& \frac{1}{n\Sigma x^2_i - (\Sigma x_i)^2}
\left(  \begin{array} {cc}
	          \Sigma_i x_i^2 & -\Sigma_i x_i \\
		 -\Sigma_i x_i &n\\
		\end{array}
		\right) 
\left(  \begin{array} {c}
	          \Sigma_i y_i \\
		 -\Sigma x_i y_i\\
		\end{array}
		\right) 
\end{align*}
So
\begin{align} \label{eq:beta0}
\beta_0 & = \frac{\Sigma x_i^2 \Sigma y_i - \Sigma x_i (\Sigma x_i y_i)}{n\Sigma x^2_i - (\Sigma x_i)^2}
\end{align}
\begin{align} \label{eq:beta1} 
\beta_1 & = \frac{n \Sigma x_i y_i -\Sigma x_i \Sigma y_i }{n\Sigma x^2_i - (\Sigma x_i)^2}
\end{align}
$\beta_1$ can also be written using the covariance
\begin{equation} \label{eq:betacovariance}
\beta_1=\frac{\sum_i^n(x_i-\bar x)(y_i-\bar y)}{\sum_i^n(x_i-\bar x)(x_i-\bar x)}
\end{equation}
And it is easy to show 
\begin{align*}
\beta_2 &=\frac{\sum_i^n(x_i-\bar x)(y_i-\bar y)}{\sum_i^n(x_i-\bar x)(x_i-\bar x)}\\
            &=\frac{\sum_i^n(x_iy_i-\bar x y_i -x_i \bar y + \bar x \bar y)}{\sum_i^n(x_i^2 -2\bar x x_i + (\bar x)^2)}\\
	    &=\frac{\sum_i^n x_iy_i-\sum_i^n \bar x y_i -\sum_i^n x_i \bar y + \sum_i^n \bar x \bar y}{\sum_i^n x_i^2 -\sum_i^n 2\bar x x_i + \sum_i^n (\bar x)^2}\\
           &=\frac{\sum_i^n x_iy_i-(\frac{1}{n}\sum_j^n x_j)(\sum_i^n y_i) -(\sum_i^n x_i)(\frac{1}{n}\sum_j^n y_j) + \sum_i^n (\frac{1}{n}\sum_j^n x_i)(\frac{1}{n}\sum_k^n y_i)}{\sum_i^n x_i^2 -\sum_i^n 2(\frac{1}{n}\sum_j^n x_j) x_i + \sum_i^n (\frac{1}{n}\sum_j^n x_j)^2}\\
 &=\frac{\sum_i^n x_iy_i-(\frac{1}{n}\sum_j^n x_j)(\sum_i^n y_i) -(\sum_i^n x_i)(\frac{1}{n}\sum_j^n y_j) + n (\frac{1}{n}\sum_j^n x_i)(\frac{1}{n}\sum_k^n y_k)}{\sum_i^n x_i^2 -\sum_i^n 2(\frac{1}{n}\sum_j^n x_j) x_i + n(\frac{1}{n}\sum_j^n x_j)^2}\\
&=\frac{\sum_i^n x_iy_i-\frac{1}{n}(\sum_i^n x_i)(\sum_j^n y_j) -\frac{1}{n}(\sum_i^n x_i)(\sum_j^n y_j) +  \frac{1}{n}(\sum_j^n x_i)(\sum_k^n y_k)}{\sum_i^n x_i^2 - \frac{2}{n}(\sum_j^n x_j) (\sum_i^nx_i) + \frac{1}{n}(\sum_j^n x_j)^2}\\
&=\frac{\sum_i^n x_iy_i-\frac{1}{n}(\sum_i^n x_i)(\sum_j^n y_j)}{\sum_i^n x_i^2 - \frac{1}{n}(\sum_j^n x_j) (\sum_i^nx_i) }\\
&=\frac{n\sum_i^n x_iy_i-(\sum_i^n x_i)(\sum_j^n y_j)}{n\sum_i^n x_i^2 - (\sum_j^n x_j) (\sum_i^nx_i) }\\
&=\frac{n\sum x_iy_i-(\sum x_i)(\sum_j^n y_j)}{n\sum x_i^2 - (\sum x_i)^2 }\\
\end{align*}
which is the same as Eq.\ref{eq:beta1}.  We can interpret $\beta$ as ratio of the covariance of x and y to the variance of x.
\subsection{Projection matrix}
Given $\hat \beta = (X^TX)^{-1}X^TY$, we have the predictor value of $y=X\beta$ 
\begin{align*}
\hat y = X(X^TX)^{-1}X^T y
\end{align*}
The matrix $P=X(X^TX)^{-1}X^T$ is a projection matrix. It projects the vector of y into the column space of X.\\
{\bf Understand the word projection}\\
Let us understand this first through geometry point of view. Consider a vector on 2 dimensional space, $V_1= (x_1, y_1)^T$, where $x_1$ and $y_1$ are the x and y component, respectively. If we project the vector V into x-line, then apparently we get $V_x= (x_1, 0)^T$, see graph below.

\includegraphics[scale = 1]{project1.png}\\
If we have a vector that is along the x axis
\begin{align*}
X = \left( \begin{array} {c }
              1  \\
              0  \\
            \end{array} \right)\\
\end{align*}
 The projection matrix of a vector into x line is 
\begin{align*}
P_x & = x(x^Tx)^{-1}x^T \\
     & = \left( \begin{array} {c}
              1 \\
              0 \\
            \end{array} \right)
 \left( \begin{array} { c  c } 
                   1 & 0\\
           \end{array} \right) \\
       & =  \left( \begin{array} { c  c } 
                   1 & 0  \\
                   0 & 0  \\
           \end{array} \right)
\end{align*}
Applying this projection matrix to any 2 dimensional vector $V$ gives $(V_x, 0)^T$. So it projects the vector into x line. 
Let us take another example. Imagine $V_1$ is vector if we project $V$ onto the line that has 45 degree angle with x axis. See below.

\includegraphics[scale = 1]{project2.png}\\

In order to calculate $V_1$,  we see
\begin{align*}
r_1 = r cos(\pi/4 - alpha) =r( \frac{\sqrt{2}}{2} \frac{y}{r} + \frac{\sqrt{2}}{2} \frac{x}{r}) = \frac{\sqrt 2}{2}y +\frac{\sqrt2}{2} x
\end{align*}
\begin{align*}
V_{1x}=r_1cos(\pi/4)=\frac{x+y}{2}\\
V_{1y}=r_1sin(\pi/4)=\frac{x+y}{2}\\
\end{align*}
After we understand this using geometry point of view, we can workout from algebra point of view. The vector we want to project onto is
\begin{align*}
i = \left( \begin{array} {c }
              1  \\
              1  \\
            \end{array} \right)\\
\end{align*}
The projection matrix of a vector into x line is 
\begin{align*}
P_x & = x(x^Tx)^{-1}x^T \\
     & = \left( \begin{array} {c}
              1 \\
              1 \\
            \end{array} \right)
\left( \left( \begin{array} {c c}
              1 & 1\\
            \end{array} \right) 
          \left( \begin{array} {c }
              1 \\
              1 \\
            \end{array} \right) \right)^{-1}
 \left( \begin{array} { c  c } 
                   1 & 1\\
           \end{array} \right) \\
       & =  \frac{1}{2}\left( \begin{array} { c  c } 
                   1 & 1  \\
                   1 & 1  \\
           \end{array} \right)
\end{align*}
Therefore we easily see
\begin{align*}
V_1 =  \frac{1}{2}\left( \begin{array} { c  c } 
                   1 & 1  \\
                   1 & 1  \\
           \end{array} \right) 
             \left( \begin{array} {c }
              x \\
              y \\
            \end{array} \right)
          = \left( \begin{array} {c }
              \frac{1}{2} (x+y) \\
              \frac{1}{2} (x+y) \\
            \end{array} \right)
\end{align*}
which is the same as what we get based on geometry.
 For n dimensional vector y, if our X matrix has rank of k, then the projection matrix P projects the vector y into k dimensional hyperplane. For example, if we define
\begin{align*}
i_N =   \left( \begin{array} {c }
              1 \\
              1 \\
             ...\\
              1
            \end{array} \right)
\end{align*}
The projection matrix P is
\begin{align*}
P = i \frac{1}{N}i^T
  = \frac{1}{N} \left( \begin{array} { c  c c c } 
                   1 & 1 & ... & 1 \\
                   1 & 1 & ... & 1 \\
                   ... & ... & ... & ... \\
                   1 & 1 & ... & 1 \\
           \end{array} \right) 
\end{align*}
{\bf Projection matrix into null space}\\
If $P$ is a projection matrix, the matrix $I - P$ is also a projection matrix. In linear regression model
\begin{align*}
y &= X\beta +\epsilon\\
P &= X(X^TX)^{-1}X^T\\
\end{align*}
Define residual vector $\hat \epsilon$
\begin{align*}
\hat \epsilon & = (I - P) y = (I - X(X^TX)^{-1}X^T)y\\
\end{align*}
And it is easy to show $\hat e$ and X are orthogonal.
\begin{align*}
X^T \hat \epsilon = X^T (I-P)y =X^T(I - X(X^TX)^{-1}X^T)y = (X^T- X^TX(X^TX)^{-1}X^T)y = 0 y = 0
\end{align*}
For the above example, we define $M = I - \frac{1}{N}i i^T$, and $My$ express the mean deviations of a vector.\\
{\bf Idempotent property of projection matrix\\}
Consider the previous example that we project a vector V onto x axis, how about we do this projection twice, we would end up the same vector $V_x$. Using a little matrix algebra, it is easy to prove that for any project matrix P, we have $PP=P$.  
\subsection{Partitioned Regression and  Regression}
\begin{align*}
y = X \boldsymbol \beta + \epsilon = X_1 \beta_1 + X_2 \beta_2 + \epsilon
\end{align*}
The normal equation is
\begin{align*}
 \left( \begin{array} { c  c } 
                   X_1^TX_1 & X_1^TX_2  \\
                   X_2^TX_1 & X_2^TX_2   \\
           \end{array} \right)
 \left( \begin{array} { c  } 
                   \hat \beta_1   \\
                   \hat \beta_2   \\
           \end{array} \right) =
\left( \begin{array} { c  } 
                   X_1^T y   \\
                   X_2^T y   \\
           \end{array} \right)
\end{align*}
If X1 and X2 are orthogonal, namely, $X_1^T X_2=0$, then 
\begin{align*}
\boldsymbol {\hat  \beta_1} = (X_1^T X_1)^{-1} X_1^Ty\\
\boldsymbol {\hat  \beta_2} = (X_2^T X_2)^{-1} X_2^Ty
\end{align*}
If X1 and X2 are not orthogonal, we can solve for $\beta_2$ in the above normal equation set and get $\beta_2$
\begin{align*}
\hat \beta_2 & = [X_2^T(I-X_1(X_1^TX_1)^{-1}X_1^T)X_2]^{-1}[X_2(I-X_1(X_1^TX_1)^{-1}X_1^T)y]\\
& = (X_2^T M_1X_2)^{-1}(X_2^T  M_1y) \\
\end{align*}
Given the fact that $M_1$ is symmetrical and idempotent, we can rewrite the above expression
\begin{align}
\hat \beta_2  & = (X_2^T M_1 M_1X_2)^{-1}(X_2^T M_1 M_1y)  \notag \\
&= (X_2^T M_1^T M_1X_2)^{-1}(X_2^TM_1^T M_1y) \notag \\
& = ((M_1 X_2)^T  M_1X_2)^{-1}((M_1 X_2)^T M_1y)  \label{eq:partial} 
\end{align}
The above uses the property that $M_1^T=M_1$ and $M_1 M_1 = M_1$\\
The $\hat \beta_2$ is also the solution of 
\begin{align*}
 M_1 Y = M_1 X_2 \beta_2 + \epsilon
\end{align*}
where $M_1 y$ is the residual of y regressed on $X_1$ and $M_1 X_2$ is the residual of $X_2$ regressed on $X_1$.
For example, in simple linear regression
\begin{align*}
Y= \beta_0 + x\beta_1
\end{align*}
Where $X_1=1_N$, so its projection matrix is $i\frac{1}{N}i^T$, and the corresponding M matrix is $I - \frac{1}{N}ii^T$.
We tries to calculate $\beta$ using partition regression. 
\begin{align*}
MY = (I-\frac{1}{N}ii^T)Y = Y-\bar Y
MX = (I-\frac{1}{N}ii^T)Y = X-\bar X
\end{align*}
Then
\begin{align}
\beta_1 = ((MX)^T(MX))^{-1}((MX)^T(MY))  = \frac{\sum_i^N(x_i - \bar x)(y_i - \bar y)}{\sum_i^N(x_i-\bar x)^2} \label{eq:beta_partition}
\end{align}
which is the same as Eq.\ref{eq:betacovariance}
\subsection{Variance componet identity}
If we define our mean projection matrix P 
\begin{align*}
P = i \frac{1}{N}i^T
\end{align*}
and silimarly we define mean deviation project matrix 
\begin{align*}
M= I-P = i\frac{1}{N}i^T
\end{align*}
We have
\begin{align*}
y= \hat y + \hat \epsilon = X\hat \beta + \hat \epsilon
\end{align*}
Multiplying M matrix on the left, we have
\begin{align*}
My= MX\hat \beta + M \hat \epsilon = MX\hat \beta +\hat \epsilon
\end{align*}
\begin{align*}
(My)^2 &= (MX\hat \beta + \hat \epsilon)^T(MX\hat \beta +  \hat \epsilon) \\
&=(\beta^T X^T M^T + \hat \epsilon^T )(MX\hat \beta + \hat \epsilon) \\
&=(MX\hat \beta)^2 + \beta^T X^T M^T \hat \epsilon+ (\beta^T X^T M^T \hat \epsilon)^T  + (\hat \epsilon)^2\\
\end{align*}
The 2nd and 3rd terms are zero because that 1)$\hat \epsilon$ has zero mean, so $M^T\hat \epsilon = M \hat \epsilon = \hat \epsilon$ and 2) $X^T\hat \epsilon = 0$, so
\begin{align*}
(My)^2 &=(MX\hat \beta)^2 + (\hat \epsilon)^2\\
\end{align*}
Rewriting the above equation using summation, we have
\begin{align*}
\sum_i (y_i - \bar y)^2 =\sum_i^N (\bar y_i - \bar {\hat y})^2 + \sum_i(y_i-\hat y)^2
\end{align*}
Define
\begin{align*}
SST & = \sum_i (y_i - \bar y)^2 \\
SSR & = \sum_i (\bar y_i - \bar {\hat y})^2 \\
SSE & = \sum_i(y_i-\hat y)^2\\
\end{align*}
SSE can also be written as
\begin{align*}
SSE & = SST- SSR\\
      & = SST - (MX\hat \beta)^2\\
      & = SST - ((MX)((MX)^T(MX))^{-1}(MX)^T(MY))^2 
\end{align*}
Let $U = MX$, and$V= MY$ then
\begin{align*}
SSE & = SST - (Z(Z^TZ)^{-1}Z^T)^2 \\
      &  = SST - (U(U^TU)^{-1}U^TV)^T (U(U^TU)^{-1}U^TV) \\
      &  = SST - V^TU(U^TU)^{-1}U^TU(U^TU)^{-1}U^TV\\
      &  = SST - V^TU(U^TU)^{-1}U^TV\\
      &  = SST - (MY)^T(MX)((MX)^T(MX))^{-1}(MX)^T(MY)\\
\end{align*}
Define
\begin{align*}
S_{xx} &= (MX)^T(MX)  = \sum_{i}^N (x_i - \bar x)^2\\
S_{xy} &= (MX)^T(MY) = \sum_{i}^N (x_i - \bar x)(y_i -\bar y)\\
\end{align*}
So
\begin{align*}
SSE = SST - S_{xy}^T S_{xx}^{-1} S_{xy}
\end{align*}

Then we have
\begin{align*}
SST = SSR + SSE
\end{align*}
\subsection{Variance of $\hat \beta$ and $\sigma^2$ estimation}
\begin{align*}
Var(\hat \beta)
=Var((X^TX)^{-1}X^T \epsilon)
=(X^TX)^{-1}X^T Var( \epsilon) ((X^TX)^{-1}X^T)^T\\
=\sigma^2 (X^TX)^{-1}X^T  X (X^TX)^{-1}
=\sigma^2 (X^TX)^{-1}
\end{align*}
The above derivation use the fact that $\epsilon$ has a normal distribution with mean 0 and variance $\sigma^2$.
For simple linear regression
\begin{align*}
Var(\hat \beta)
=\frac{\sigma^2}{n\Sigma x_i^2 - (\Sigma x_i)^2} \left(  \begin{array} {cc}
	          \Sigma_i x_i^2 & -\Sigma_i x_i \\
		 -\Sigma_i x_i &n\\
		\end{array}
		\right) 
\end{align*}
\begin{align*}
Var(\hat \beta_0) =\frac{\Sigma x_i^2 \sigma^2}{n\Sigma x_i^2 - (\Sigma x_i)^2} 
\end{align*}
\begin{align*}
Var(\hat \beta_1) =\frac{n \sigma^2}{n\Sigma x_i^2 - (\Sigma x_i)^2} 
\end{align*}
Try
\begin{align*}
\Sigma (x_i- \bar x)^2 = \Sigma(x_i^2 - 2 \bar x x_i+ \bar x^2)
= \Sigma_i(x_i^2 - 2 (\Sigma_j \frac{x_j}{n}) x_i + \frac{ (\Sigma_j x_j)^2} {n^2})\\
=\Sigma_ix_i^2 - \frac{2}{n} (\Sigma_i x_i)^2 + \frac{ (\Sigma_i x_i)^2} {n}
= \Sigma_i x^2_i - \frac{1}{n} (\Sigma x_i)^2
\end{align*}
So
\begin{align*}
Var(\hat \beta_0) =\frac{\Sigma x_i^2 \sigma^2}{n\Sigma (x_i- \bar x)^2 } 
\end{align*}
\begin{align*}
Var(\hat \beta_1) =\frac{n \sigma^2}{n\Sigma (x_i- \bar x)^2} = \frac{\sigma^2}{\Sigma (x_i- \bar x)^2}
\end{align*}
\begin{align*}
SSE
&=\Sigma_i (y-\hat y_i)^2 \\
& = (Y-X\beta)^T(Y-X\beta) \\
& = (Y-X(X^TX)^{-1}X^TY)^T(Y-X(XTX)-1X^TY) \\
& = (Y-PY)^T(Y-PY) \\
& = Y^T(1-P)^T(1-P)Y = Y^T(1-P)Y \\
& = (X\beta + \epsilon) ^T (1-P)(X\beta+\epsilon) \\
& = \beta^T X^T (1-P)X\beta + 2\beta^T X^T X^T(I-P)\epsilon 
   + \epsilon^T(I-H)\epsilon 
\end{align*}
\begin{align*}
E[SSE] = E[\epsilon^T(I-P)\epsilon] = E[\epsilon^T \epsilon] trace(I-H)
          = \sigma^2(n-k)\\
\end{align*}
We obtain the unbiased estimator of $\sigma^2$\\
\begin{align*}
\hat \sigma^2 = \frac{SSE}{n-k}
\end{align*}
Therefore the estimator of variance of $\beta$
\begin{align*}
\hat Var(\hat \beta_i) = \hat \sigma^2(X^TX)^{-1}_{ii}
\end{align*}
and the stanard error of $\beta_i$ is
\begin{align*}
SE(\hat \beta_i) = \sqrt{ \hat \sigma^2(X^TX)^{-1}_{ii}}
\end{align*}
\section{Properties of Least Square Estimators}
When we have a estimator, we need to evaluate how good our estimator is? A few questions we can ask is 1): how far is the value of our estimator away from the true value, even in the ideal case when the sample size is inifinite? 2) when 1) is true, with finite sample size, does the value of our estimator approach to the true value as the sample size increase? In other words, does the estimator converge to the true value as sample size goes to inifinity? 3) when 1) and 2) is true, as the sample size increase, how fast does our estimator converges to true value? 4) with 1) 2) and 3), what is the asymptotic distribution of the estimator? If the distribution is normal, it can be used to do interval estimation such as confidence interval. The 1st question defines unbiasness, the 2nd one defines consistency, and the 3rd one defines efficiency.\\ 
\subsection{Unbiasness}
{\bf Unbiased}\\
\begin{align*}
\hat \beta & = (X^T X)^{-1}X^TY\\
               & = (X^T X)^{-1}X^T(X \beta + \epsilon)\\
               & = (X^T X)^{-1}X^T X \beta   + (X^T X)^{-1}X^T \epsilon\\
               & = \beta +  (X^T X)^{-1}X^T \epsilon\\
\end{align*}
Then the expectation of $\hat \beta$ condition on X is
\begin{align*}
E[{\hat \beta|X}] = \beta + (X^TX)^{-1} X^T E(\epsilon|X)\\t
\end{align*}
The last term is zero by assuption of linear regression. So 
\begin{align*}
E[\hat \beta] = \beta
\end{align*}
The expectation of the estimator is the same as true value, this is called {\bf unbiased}. \\
{\bf Bias due to omission of relevant variables}\\
Suppose we have a model
\begin{align*}
y = X_1 \beta_1 + X_2 \beta_2 + \epsilon
\end{align*}
If we regression y on $X_1$ only, our estimator is
\begin{align*}
\hat \beta_1 = (X_1^T X_1)^{-1} X_1^T y = \beta_1 + (X_1^TX_1)^{-1}X_1^TX_2\beta_2 + (X_1^TX_1)^{-1}X_1^T\epsilon
\end{align*}
On the second term, we see unless 1)$X_1$ and $X_2$ are orthogonal, or 2)$\beta_2$ =0,  $\beta_1$ is biased.\\
\subsection{Consistency}
The unbiasness gives us a metric of measuring how good our esitimator is, from population perspetive. In reality, as our sample size is finite, we need ask ourselves does our estimator converges to true value when sample size is sufficiently large. We know
\begin{align*}
\hat \beta = \beta + (X^TX)^{-1}X^T\epsilon
\end{align*}

\begin{align*}
\hat \beta &= \beta + ({X^TX})^{-1} {X^T \epsilon} \\
               &= \beta + ( \Sigma_{i=1}^N X_i X_i^T)^{-1} X^T \epsilon \\
               &= \beta + ( \Sigma_{i=1}^N  \frac{1}{N}{X_iX_i^T})^{-1}(\frac{X^T \epsilon}{N})
\end{align*}
To show $\hat \beta$ converges to $\beta$, we need to show two things:\\
(1)$\frac{1}{n}\sum_i{X_iX_i^T}$  converges to Q in probability when N is large. Also the inverse of Q exists.\\
(2)$\frac{X^T \epsilon}{N}$ converges to zero in probability when N is large.\\
For (1), we write
\begin{align*}
Q^{(n)} = \frac{1}{n}\sum_i{X_iX_i^T}
\end{align*}
which is a $K \times K$ matrix. Its element $Q_{kl}$ is
\begin{align*}
Q^{(n)}_{kl} = \frac{1}{n} \sum_i x_{ik} x_{il}
\end{align*}
Based on Law of large numbers, $Q^{n}_{kl}$ converges to its expectation value $E[x_{ik}x_{il}]$. Let
\begin{align*}
E[x_{ik}x_{il}]=Q_{kl}
\end{align*}
So each element of $Q^{(n)}$ converges $Q_{kl}$. Therefore, $Q^{(n)}$ converges to Q. To show the inverse of Q exists, we can show Q is a symmetric positive definite matrix. For any k dimenional vector v, we have
\begin{align*}
v^TQv = E[v^T x_i x_i^T v]
\end{align*}
Since $v^T x_i = \sum_k v_k x_{ik} = x_i^T v$ which is a scalar, so
\begin{align*}
v^TQv = E[v^T x_i x_i^T v] = E[(v^T x_i)^2]
\end{align*} 
if for any i, $x_{ik}$ are linear independent, in other words, no multicollinearity. Then $v^T x_{i} =\sum_k v_k x_{ik} \neq 0$ and $(v^T x_i)^2>0$. Therefore Q is SPD matrix and its inverse exists.
\begin{align*}
&\frac{X^T \epsilon}{N}\\ 
 =& \left(\begin{array} {c}
       \frac{1}{N}\Sigma_{1}^{N}x_{i1}\epsilon_i \\
       \frac{1}{N}\Sigma_{1}^{N}x_{i2}\epsilon_i \\
       \frac{1}{N}\Sigma_{1}^{N}x_{i3}\epsilon_i \\
       ... \\ 
      \frac{1}{N}\Sigma_{1}^{N}x_{ik}\epsilon_i\\
     \end{array} \right)\\
=& \frac{1}{N}\Sigma_{i=1}^{N} X_i\epsilon_i = \bar w\\
\end{align*}
Where $\bar w$ is a k $\times$ 1 vector. To see the asymptotical behavior of $w$, we consider its mean and asymptotical varance. The mean is
\begin{align*}
E[w_i]=E_X[E[w_i|x_i]] = E_X[X_iE[\epsilon|X_i]] = 0
\end{align*}
\begin{align*}
Var[\bar w]=E[Var[\bar w|X]]+Var[E[\bar w|X]] = E[Var[\bar w|X]]+0 = E[Var[\bar w|X]]
\end{align*}
\begin{align*}
Var[\bar w|X] = E[\bar w \bar w^T|X]=\frac{1}{n}X^TE[\epsilon \epsilon^T]X\frac{1}{n} = \frac{\sigma^2}{n}\frac{X^T X}{n}
\end{align*}
\begin{align*}
E[Var[\bar w|X]] = \frac{\sigma^2}{n}E(\frac{X^TX}{n})
\end{align*}
We have shown that $X^TX = \sum_i x_i x_i^T$, so 
\begin{align*}
\frac{X^TX}{n} = \frac{1}{n}\sum_i x_i x_i^T 
\end{align*}

When $\frac{X^TX}{n}$ converges to Q, 
\begin{align*}
E[Var[\bar w|X]] = 0
\end{align*} 
So $\bar w$ converges to $\boldsymbol 0(k \times 1)$ vector. Then when N is sufficiently large, $\hat \beta$ converges to $\beta$. This is the proof of consistency.\\
There are certain conditions in which the estimators become inconsitent.\\
1) X is not full rank, or X has multicollinearity
2) $cov[X, \epsilon] \neq 0$
\subsection{Efficiency}
The least equare estimator has the smallest varaince, and this can be proved by Gauss-Markov theorem. 
\subsection{Multicollinearity}
Suppose we have a regression model that contains two parameters
\begin{align*}
y = \beta_0 + X_1\beta_1 + X_2 \beta_2
\end{align*}
From above, we know variance of ${\bf \hat \beta}$ is
\begin{align*}
Var(\hat \beta) = \frac{\sigma^2}{(X^TX)^{-1}}
\end{align*}
When X only contains 2 variables, $X=(X_1, X_2)$
\begin{align*}
Var(\hat \beta_1) = \sigma^2 \frac{S_{22}}{S_{11}S_{22}-S_{12}^2} = \frac{1}{S_{11}(1-\frac{S^2_{12}}{S_{11}S_{22}})}= \frac{1}{S_{11}(1-r_{12}^2)} \\
Var(\hat \beta_2) = \sigma^2 \frac{S_{11}}{S_{11}S_{22}-S_{12}^2} = \frac{1}{S_{22}(1-\frac{S^2_{12}}{S_{11}S_{22}})}= \frac{1}{S_{22}(1-r_{12}^2)} \\
\end{align*}
Where
\begin{align*}
S_{11} & =\Sigma(x_{1i}-\hat x_1)^2\\
S_{22} & =\Sigma(x_{2i}-\hat x_2)^2\\
S_{12} & =\Sigma(x_{1i}-\hat x_1)(x_{2i}-\hat x_2)\\
r_{12} & = \frac{S_{12}}{\sqrt{S_{11}S_{22}}}\\
\end{align*}
$r_{12}$ is the correlation coefficient. In extreme case, when $X_1$ and $X_2$ are perfectly correlated, the variance becomes infinite.\\
\section{Model Testing}
\subsection{Lagrange Multiplier(LM) test}
\subsubsection{Constrained Maximum Likelihood Estimation}
Consider a parametric model with likelihood function $L(\theta)$ for $\theta \in \Theta \subset \mathbb{R}^p$. We wish to test $q$ restrictions:

\[
H_0: g(\theta) = 0,
\]
where $g: \mathbb{R}^p \to \mathbb{R}^q$ is continuously differentiable and $q \leq p$.

The constrained maximum likelihood estimator (CMLE) solves:
\[
\max_{\theta} \ell(\theta) \quad \text{s.t.} \quad g(\theta) = 0,
\]
where $\ell(\theta) = \log L(\theta)$.
\subsubsection{Lagrangian and First-Order Conditions}
Form the Lagrangian:
\[
\mathcal{L}(\theta, \lambda) = \ell(\theta) - \lambda' g(\theta),
\]
where $\lambda \in \mathbb{R}^q$ is the vector of Lagrange multipliers.

The first-order conditions are:
\begin{align}
    \frac{\partial \mathcal{L}}{\partial \theta} &= s(\theta) - G(\theta)'\lambda = 0, \label{foc_theta} \\
    \frac{\partial \mathcal{L}}{\partial \lambda} &= -g(\theta) = 0, \label{foc_lambda}
\end{align}
where:
\begin{itemize}
    \item $s(\theta) = \frac{\partial \ell(\theta)}{\partial \theta}$ is the $p \times 1$ score vector.
    \item $G(\theta) = \frac{\partial g(\theta)}{\partial \theta}$ is the $q \times p$ Jacobian matrix of constraints.
\end{itemize}

Let $(\tilde{\theta}, \tilde{\lambda})$ denote the solution to (\ref{foc_theta}) and (\ref{foc_lambda}).
\subsubsection{Statistical Interpretation of Lagrange Multipliers}
\begin{enumerate}
\item Sample vs. Population Lagrange Multipliers
\begin{itemize}
    \item $\tilde{\lambda}$ is the \textbf{sample Lagrange multiplier}: a numerical value computed from the data.
    \item Under $H_0$, the \textbf{population Lagrange multiplier} $\lambda_0 = 0$. Why? If the constraint holds in population, imposing it does not change the optimum, so its "shadow price" is zero.
\end{itemize}

Thus, we can write:
\[
\tilde{\lambda} = \lambda_0 + \text{sampling error} = \text{sampling error} \quad \text{under } H_0.
\]

\item Distribution of $\tilde{\lambda}$ Under $H_0$
From (\ref{foc_theta}), at the constrained estimate:
\[
s(\tilde{\theta}) = G(\tilde{\theta})' \tilde{\lambda}. \tag{1}
\]

Under regularity conditions and $H_0$, as $n \to \infty$:
\begin{itemize}
    \item $\tilde{\theta} \stackrel{p}{\to} \theta_0$, where $\theta_0$ is the true parameter.
    \item By the Central Limit Theorem for the score:
    \[
    \frac{1}{\sqrt{n}} s(\theta_0) \stackrel{d}{\to} N(0, \mathcal{I}(\theta_0)),
    \]
    where $\mathcal{I}(\theta_0)$ is the Fisher information matrix.
    \item Applying the delta method and using (1):
    \[
    \frac{1}{\sqrt{n}} \tilde{\lambda} \stackrel{d}{\to} N\left(0, [G(\theta_0) \mathcal{I}(\theta_0)^{-1} G(\theta_0)']^{-1}\right).
    \]
\end{itemize}

Therefore, under $H_0$:
\[
\tilde{\lambda} \stackrel{a}{\sim} N\left(0, \frac{1}{n} [G(\theta_0) \mathcal{I}(\theta_0)^{-1} G(\theta_0)']^{-1}\right).
\]
\end{enumerate}

\subsubsection{Constructing the LM Test Statistic}
\begin{enumerate}
%\subsubsection{Quadratic Form in $\tilde{\lambda}$}
\item{Quadratic Form in $\tilde{\lambda}$}
Since $\tilde{\lambda} \sim N(0, \Sigma_\lambda)$ under $H_0$, the quadratic form:
\[
\tilde{\lambda}' \Sigma_\lambda^{-1} \tilde{\lambda} \sim \chi^2_q.
\]

From the asymptotic variance above:
\[
\Sigma_\lambda = \frac{1}{n} [G(\theta_0) \mathcal{I}(\theta_0)^{-1} G(\theta_0)']^{-1}.
\]
Hence:
\[
n \cdot \tilde{\lambda}' G(\theta_0) \mathcal{I}(\theta_0)^{-1} G(\theta_0)' \tilde{\lambda} \stackrel{d}{\to} \chi^2_q.
\]

\item{Connection to the Score Vector}
Using $s(\tilde{\theta}) = G(\tilde{\theta})' \tilde{\lambda}$ from (1):
\[
n \cdot \tilde{\lambda}' G(\theta_0) \mathcal{I}(\theta_0)^{-1} G(\theta_0)' \tilde{\lambda} 
= n \cdot s(\tilde{\theta})' \mathcal{I}(\theta_0)^{-1} s(\tilde{\theta}) + o_p(1).
\]

Replacing $\mathcal{I}(\theta_0)$ with a consistent estimator $\hat{\mathcal{I}}$ (e.g., observed information at $\tilde{\theta}$), we obtain the \textbf{Lagrange Multiplier (LM) test statistic}:
\[
\boxed{\text{LM} = s(\tilde{\theta})' \hat{\mathcal{I}}^{-1} s(\tilde{\theta}) \stackrel{d}{\to} \chi^2_q}.
\]
\end{enumerate}

\subsection{Example: Testing $\beta_2 = 0$ in a Linear Model}

Consider the linear regression model:
\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u, \quad u \sim N(0, \sigma^2 I)
\]
with $n$ observations. We want to test $H_0: \beta_2 = 0$.

Let $\theta = (\beta_0, \beta_1, \beta_2)' \in \mathbb{R}^3$.

\subsubsection{Constraint, Maximum Likelihood and Lagrangian}
The constraint is $g(\theta) = \beta_2 = 0$. Here $q=1$, and:
\[
G(\theta) = \frac{\partial g}{\partial \theta} = \begin{pmatrix} 0 & 0 & 1 \end{pmatrix}.
\]

The log-likelihood is:
\[
\ell(\theta) = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_{1i} - \beta_2 x_{2i})^2.
\]

The Lagrangian is:
\[
\mathcal{L}(\theta, \lambda) = \ell(\theta) - \lambda \beta_2,
\]
where $\lambda$ is a scalar Lagrange multiplier.

\subsubsection{First-Order Conditions}
\begin{align}
    \frac{\partial \mathcal{L}}{\partial \beta_0} &= \frac{1}{\sigma^2}\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_{1i} - \beta_2 x_{2i}) = 0, \label{foc_b0} \\
    \frac{\partial \mathcal{L}}{\partial \beta_1} &= \frac{1}{\sigma^2}\sum_{i=1}^n x_{1i}(y_i - \beta_0 - \beta_1 x_{1i} - \beta_2 x_{2i}) = 0, \label{foc_b1} \\
    \frac{\partial \mathcal{L}}{\partial \beta_2} &= \frac{1}{\sigma^2}\sum_{i=1}^n x_{2i}(y_i - \beta_0 - \beta_1 x_{1i} - \beta_2 x_{2i}) - \lambda = 0, \label{foc_b2} \\
    \frac{\partial \mathcal{L}}{\partial \lambda} &= -\beta_2 = 0. \label{foc_lambda_ex}
\end{align}

From (\ref{foc_lambda_ex}), $\beta_2 = 0$ under the constraint.

\subsubsection{Constrained Estimates}
Let $\tilde{\theta} = (\tilde{\beta}_0, \tilde{\beta}_1, 0)'$ be the constrained MLE. 
Substituting $\beta_2 = 0$ into (\ref{foc_b0}) and (\ref{foc_b1}) gives the OLS estimates from the restricted model:
\[
y = \beta_0 + \beta_1 x_1 + u.
\]
These are simply:
\[
\begin{pmatrix}
\tilde{\beta}_0 \\
\tilde{\beta}_1
\end{pmatrix} = (W'W)^{-1}W'y,
\]
where $W = [\mathbf{1}, x_1]$ is the $n \times 2$ matrix of intercept and $x_1$.

\subsubsection{Solving for $\tilde{\lambda}$}
From (\ref{foc_b2}), with $\beta_2 = 0$:
\[
\tilde{\lambda} = \frac{1}{\sigma^2}\sum_{i=1}^n x_{2i}(y_i - \tilde{\beta}_0 - \tilde{\beta}_1 x_{1i}) = \frac{1}{\sigma^2}\sum_{i=1}^n x_{2i} \tilde{u}_i,
\]
where $\tilde{u}_i$ are residuals from the restricted regression.

In matrix form:
\[
\tilde{\lambda} = \frac{1}{\sigma^2} x_2' \tilde{u},
\]
where $x_2$ is the $n \times 1$ vector of $x_{2i}$ and $\tilde{u} = y - W\tilde{\beta}$.

\subsubsection{Distribution of $\tilde{\lambda}$ Under $H_0$}

Under $H_0: \beta_2 = 0$, the true model is $y = \beta_0 + \beta_1 x_1 + u$. Then:
\begin{itemize}
    \item $\tilde{\beta}_0, \tilde{\beta}_1$ are consistent for $\beta_0, \beta_1$.
    \item $\tilde{u}_i = u_i - (\tilde{\beta}_0 - \beta_0) - (\tilde{\beta}_1 - \beta_1)x_{1i}$.
\end{itemize}

Since $\mathbb{E}[x_{2i} u_i] = 0$ (by exogeneity) under $H_0$:
\[
\mathbb{E}[\tilde{\lambda}] = 0.
\]

The variance can be computed. Write $M_W = I - W(W'W)^{-1}W'$, the annihilator matrix for $W$. Then $\tilde{u} = M_W y = M_W u$ under $H_0$. Thus:
\[
\tilde{\lambda} = \frac{1}{\sigma^2} x_2' M_W u.
\]
Since $u \sim N(0, \sigma^2 I)$:
\[
\tilde{\lambda} \sim N\left(0, \frac{1}{\sigma^2} x_2' M_W x_2\right).
\]
Note $x_2' M_W x_2$ is the residual sum of squares from regressing $x_2$ on $W$.

Thus, under $H_0$:
\[
\frac{\tilde{\lambda}^2}{\frac{1}{\sigma^2} x_2' M_W x_2} = \sigma^2 \frac{\tilde{\lambda}^2}{x_2' M_W x_2} \sim \chi^2_1.
\]

\subsubsection{Score Vector at Constrained Estimate}
The score vector is:
\[
s(\theta) = \frac{1}{\sigma^2} \begin{pmatrix}
\sum (y_i - \beta_0 - \beta_1 x_{1i} - \beta_2 x_{2i}) \\
\sum x_{1i}(y_i - \beta_0 - \beta_1 x_{1i} - \beta_2 x_{2i}) \\
\sum x_{2i}(y_i - \beta_0 - \beta_1 x_{1i} - \beta_2 x_{2i})
\end{pmatrix}.
\]

At $\tilde{\theta} = (\tilde{\beta}_0, \tilde{\beta}_1, 0)'$:
\[
s(\tilde{\theta}) = \frac{1}{\sigma^2} \begin{pmatrix}
0 \\
0 \\
\sum x_{2i} \tilde{u}_i
\end{pmatrix},
\]
because the first two equations are satisfied by $\tilde{\beta}_0, \tilde{\beta}_1$ (from FOCs \ref{foc_b0}, \ref{foc_b1}).

Note that the third element equals $\tilde{\lambda}$, consistent with the general relation $s(\tilde{\theta}) = G(\tilde{\theta})' \tilde{\lambda}$.

\subsubsection{Information Matrix}
The information matrix for this normal linear model is:
\[
\mathcal{I}(\theta) = \frac{1}{\sigma^2} \begin{pmatrix}
n & \sum x_{1i} & \sum x_{2i} \\
\sum x_{1i} & \sum x_{1i}^2 & \sum x_{1i}x_{2i} \\
\sum x_{2i} & \sum x_{1i}x_{2i} & \sum x_{2i}^2
\end{pmatrix} = \frac{1}{\sigma^2} X'X,
\]
where $X = [\mathbf{1}, x_1, x_2]$.

\subsubsection{LM Statistic}
The LM statistic is:
\[
\text{LM} = s(\tilde{\theta})' \mathcal{I}(\tilde{\theta})^{-1} s(\tilde{\theta}).
\]

Since only the third element of $s(\tilde{\theta})$ is non-zero, and $\mathcal{I}^{-1} = \sigma^2 (X'X)^{-1}$, we have:
\[
\text{LM} = \frac{1}{\sigma^4} (0, 0, \sum x_{2i}\tilde{u}_i) \cdot \sigma^2 (X'X)^{-1} \cdot \begin{pmatrix} 0 \\ 0 \\ \sum x_{2i}\tilde{u}_i \end{pmatrix}.
\]

Let $v = (0, 0, 1)'$ and note $\sum x_{2i}\tilde{u}_i = x_2'\tilde{u}$. Then:
\[
\text{LM} = \frac{1}{\sigma^2} (x_2'\tilde{u}) \cdot v'(X'X)^{-1} v \cdot (x_2'\tilde{u}).
\]

But $v'(X'X)^{-1} v = [(X'X)^{-1}]_{33}$, the (3,3) element of $(X'X)^{-1}$, which is $(x_2' M_W x_2)^{-1}$ (the inverse of the residual variance from regressing $x_2$ on $W$).

Thus:
\[
\text{LM} = \frac{(x_2'\tilde{u})^2}{\sigma^2 \cdot (x_2' M_W x_2)} = \frac{\tilde{\lambda}^2}{\frac{1}{\sigma^2} x_2' M_W x_2},
\]
exactly the quadratic form in $\tilde{\lambda}$ derived earlier.

\subsubsection{Connection to Usual t-Test}

%\subsection{Unconstrained OLS}
The unconstrained OLS estimate of $\beta_2$ is:
\[
\hat{\beta}_2 = \frac{x_2' M_W y}{x_2' M_W x_2}.
\]
Under $H_0$, $\hat{\beta}_2 \sim N\left(0, \frac{\sigma^2}{x_2' M_W x_2}\right)$.

The t-statistic is:
\[
t = \frac{\hat{\beta}_2}{\text{SE}(\hat{\beta}_2)} = \frac{\hat{\beta}_2}{\sqrt{\hat{\sigma}^2/(x_2' M_W x_2)}},
\]
where $\hat{\sigma}^2$ is the error variance estimator from the unconstrained regression.

%\subsection{Relation to $\tilde{\lambda}$}
Note that:
\[
x_2' \tilde{u} = x_2' M_W y = (x_2' M_W x_2) \hat{\beta}_2.
\]
Thus:
\[
\tilde{\lambda} = \frac{1}{\sigma^2} (x_2' M_W x_2) \hat{\beta}_2.
\]

The LM statistic becomes:
\[
\text{LM} = \frac{(x_2' M_W x_2)^2 \hat{\beta}_2^2 / \sigma^4}{\frac{1}{\sigma^2} x_2' M_W x_2} = \frac{(x_2' M_W x_2) \hat{\beta}_2^2}{\sigma^2}.
\]

If we use the true $\sigma^2$, then $\text{LM} = \hat{\beta}_2^2 / \text{Var}(\hat{\beta}_2) \sim \chi^2_1$, and:
\[
\text{LM} = t^2.
\]

In practice, we estimate $\sigma^2$ from the constrained regression as $\tilde{\sigma}^2 = \frac{1}{n}\sum \tilde{u}_i^2$, and the LM statistic becomes $nR^2$ from the auxiliary regression of $\tilde{u}$ on $x_1$ and $x_2$.

\subsubsection{Summary}

For testing $H_0: \beta_2 = 0$:
\begin{itemize}
    \item The Lagrange multiplier $\tilde{\lambda} = \frac{1}{\sigma^2}\sum x_{2i}\tilde{u}_i$ measures the marginal increase in log-likelihood from relaxing $\beta_2 = 0$.
    \item Under $H_0$, $\tilde{\lambda} \sim N\left(0, \frac{1}{\sigma^2} x_2' M_W x_2\right)$.
    \item The LM statistic $\text{LM} = \frac{\tilde{\lambda}^2}{\text{Var}(\tilde{\lambda})} \sim \chi^2_1$.
    \item This equals $s(\tilde{\theta})' \mathcal{I}(\tilde{\theta})^{-1} s(\tilde{\theta})$, connecting constrained optimization to hypothesis testing.
    \item When $\sigma^2$ is known, $\text{LM} = t^2$, showing equivalence to the usual t-test.
\end{itemize}

This example illustrates the general principle: LM tests check whether the score (gradient of the likelihood) at the constrained estimate is statistically zero, which is equivalent to testing whether the Lagrange multipliers (shadow prices of constraints) are zero.

\subsubsection{Example: Testing constraint $R\beta = r$ in Linear Model }

Consider $y = X\beta + u$, $u \sim N(0, \sigma^2 I)$, with constraint $R\beta = r$.

\begin{enumerate}
\item{Constrained MLE}
%\subsection{Constrained MLE}
The Lagrangian is:
\[
\mathcal{L}(\beta, \lambda) = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}(y-X\beta)'(y-X\beta) - \lambda'(R\beta - r).
\]
FOCs:
\begin{align*}
    \frac{\partial \mathcal{L}}{\partial \beta} &= \frac{1}{\sigma^2}X'(y-X\beta) - R'\lambda = 0, \\
    \frac{\partial \mathcal{L}}{\partial \lambda} &= -(R\beta - r) = 0.
\end{align*}

The constrained estimator is $\tilde{\beta}$ with $R\tilde{\beta} = r$, and:
\[
\tilde{\lambda} = \frac{1}{\sigma^2} [R(X'X)^{-1}R']^{-1} (R\hat{\beta}_{OLS} - r),
\]
where $\hat{\beta}_{OLS} = (X'X)^{-1}X'y$.

\item{Distribution Under $H_0$}
%\subsection{Distribution Under $H_0$}
Under $H_0: R\beta = r$, and assuming $\sigma^2$ known:
\[
\tilde{\lambda} \sim N\left(0, \frac{1}{\sigma^2} [R(X'X)^{-1}R']^{-1}\right).
\]

Thus:
\[
\sigma^2 \tilde{\lambda}' R(X'X)^{-1}R' \tilde{\lambda} \sim \chi^2_q.
\]

\item{Equivalence to Score Form}
%\subsection{Equivalence to Score Form}
The score at $\tilde{\beta}$ is:
\[
s(\tilde{\beta}) = \frac{1}{\sigma^2} X'(y-X\tilde{\beta}) = R'\tilde{\lambda}.
\]
The information matrix is $\mathcal{I}(\beta) = \frac{1}{\sigma^2} X'X$.

Then:
\[
\text{LM} = s(\tilde{\beta})' \mathcal{I}(\tilde{\beta})^{-1} s(\tilde{\beta}) = \sigma^2 \tilde{\lambda}' R(X'X)^{-1}R' \tilde{\lambda},
\]
identical to the quadratic form in $\tilde{\lambda}$.
\end{enumerate}

\subsubsection{Key Insights}
\begin{enumerate}
\item{Why Test $\lambda = 0$?}
\begin{itemize}
    \item $\lambda$ measures the marginal increase in log-likelihood from relaxing the constraint.
    \item If $H_0$ is true, relaxing the constraint should not improve the likelihood significantly $\Rightarrow \lambda$ should be near zero.
    \item A large $|\tilde{\lambda}|$ suggests the constraint is costly, evidence against $H_0$.
\end{itemize}

\item{Advantages of the Score Form}
%\subsection{Advantages of the Score Form}
While we \textit{could} directly test $\tilde{\lambda} = 0$, the score form $s(\tilde{\theta})$ is preferred because:
\begin{enumerate}
    \item \textbf{Invariance}: $\text{LM} = s(\tilde{\theta})' \hat{\mathcal{I}}^{-1} s(\tilde{\theta})$ is invariant to reparameterization of constraints, while $\tilde{\lambda}$ is not.
    \item \textbf{Computational simplicity}: Often $s(\tilde{\theta})$ is easier to compute (e.g., via auxiliary regressions like White's test).
    \item \textbf{Unified theory}: The score vector appears in other contexts (Cramér-Rao bound, MLE asymptotics).
\end{enumerate}

\item{The Big Picture}
%\subsubsection{The Big Picture}
The LM test elegantly connects:
\begin{itemize}
    \item \textbf{Constrained optimization} (Lagrange multipliers)
    \item \textbf{Asymptotic distribution theory} (CLT for scores)
    \item \textbf{Quadratic forms} (chi-square distributions)
\end{itemize}

It tests whether the gradient of the likelihood at the constrained estimate is statistically zero—which is exactly what we expect if the constraints are true in population.
\end{enumerate}

Suppose we have two models, one is restriced, the other is unrestricted:
Restricted(R): $y = X_1 \beta_1 + \epsilon$\\
Unrestricted (U): $y = X_1 \beta_1 + X_2 \beta_2 + \epsilon$\\
Given the unrestricted model, the likehood function is 
\begin{align*}
L(\beta_1, \beta_2, \sigma^2) = \frac{1}{(2\pi \sigma^2)^{n/2}}exp(-\frac{1}{2\sigma^2}(y-X_1\beta_1-X_2\beta_2)^T(y-X_1\beta_1-X_2\beta_2))
\end{align*}
\begin{align*}
S_2 = \frac{\partial L}{\partial \beta_2} =\frac{1}{\sigma^2}X_2^T(y - X_1\beta_1-X_2\beta_2)
\end{align*}
When $\beta_2=0$, define
\begin{align*}
M_1 = I - X_1(X^T_1X_1)^{-1}X^T_1
\end{align*}
and $M_1X_1=0$.
\begin{align*}
S_2 =\frac{1}{\sigma^2}X_2^T(y-X\hat \beta_1)=\frac{1}{\sigma^2}X_2^TM_1y = \frac{1}{\sigma^2}X_2^TM_1(X_1 \beta_1 + \epsilon) = \frac{1}{\sigma^2}X_2^TM_1 \epsilon
\end{align*}
The last equal sign uses the fact $M_1X_1 = 0$.
\begin{align*}
 Var(X_2^TM_1 \epsilon) & = Var(X_2^TM_1 \epsilon) \\
             & =  X_2^TM_1Var(\epsilon)(X_2^TM_1)^T \\
             & =  X_2^TM_1 M_1^T X_2Var(\epsilon) \\
             & = \sigma^2  X_2^TM_1X_2 \\
\end{align*}
Define
\begin{align*}
V =  X_2^TM_1X_2
\end{align*}
Then
\begin{align*}
Var(X_2^T M_1 \epsilon) = \sigma^2V
\end{align*}
So $X_2^TM_1\epsilon$ follows normal distribution with mean 0 and variance $\sigma^2X_2^TM_1X_2$. Define
\begin{align*}
Z=\frac{X_2^TM_1\epsilon}{\sqrt{\sigma^2X_2^TM_1X_2}} = \frac{S_2}{\sqrt{\sigma^2V}}
\end{align*}
then Z follows standard normal distribution. The {\bf Lagrange Multiplier (LM) test} is defined
\begin{align*}
LM =  Z^2  = \frac{(X_2^TM_1\epsilon)^2}{\sigma^2X_2^TM_1X_2} = \frac{S_2^2}{\sigma^2 V}
\end{align*}
which follows $\chi^2$ distribution with degree of freedom 1.\\
{\bf F test}\\
We define
\begin{align*}
SSE_U = ||y -X_1\hat\beta_1 - X_2\hat\beta_2||^2\\
SSE_R = ||y -X_1\hat\beta_1 ||^2
\end{align*}
F test is defined as
\begin{align*}
F = \frac{\frac{Extra \hspace{1mm} expalined \hspace{1mm} variation}{Degree \hspace{1mm} of \hspace{1mm} Freedom}}{\frac{Remaining \hspace{1mm}  unexplained \hspace{1mm} variation}{Degree \hspace{1mm} of \hspace{1mm} Freedom}} = \frac{SSE_R - SSE_U}{\frac{SSE_U}{n-1}}
\end{align*}
Let $X=(X_1, X_2)$, and we define two projection matrices
\begin{align*}
P_U = X(X^TX)^{-1}X^T\\
P_R = X_1(X_1^TX_1)^{-1}X_1^T
\end{align*}
\begin{align*}
SSR_R-SSR_U=y^TP_Ry-y^TP_Uy = y^T(P_R-P_U)y
\end{align*}
recall
\begin{align*}
\hat \beta_2 = (X_2^T M_1 X_2)^{-1}X_2^TM_1 y
\end{align*}
The corresponding projection matrix is
\begin{align*}
M_1X_2 (X_2^T M_1 X_2)^{-1}X_2^TM_1
\end{align*}
The $SSR_R$ - $SSR_U$ is the additional variance explained by $X_2$ after removing the linear space of $X_1$ on $X_2$.  This means the projection matrix corresponding to $\beta_2$ is $P_U$ - $P_R$. So we can get $P_U$ - $P_R$ using the interpretation of projection matrix instead solving for the projection matrix itself.
\begin{align*}
P_U-P_R & =M_1X_2(X_2^TM_1X_2)^{-1}X_2^TM_1\\
\end{align*}
The extra expained sum of squares by the unrestricted model is
\begin{align*}
SSR_R-SSR_U & = y^T(P_R-P_U)y = (X_2^TM_1y)^T(X_2^TM_1X_2)^{-1}X_2^TM_1y
\end{align*}
with 1 degree of freedom as $X_2$ only contains 1 parameter.
\begin{align*}
F = \frac{SSR_R - SSR_U}{\frac{SSR_U}{n-k}} = \frac{(X_2M_1y)^T(X_2^TM_1y)}{\hat \sigma^2 (X_2^TM_1X_2)} =LM
\end{align*}
We see that F test and LM test are equivalent.\\
{\bf Wald Test}\\
Recall the estimator for $\beta_2$ in Eq.\ref{eq:partial},
\begin{align*}
\hat \beta_2  =  (X_2^T  M_1X_2)^{-1}(X_2^T M_1y)  
\end{align*}
Substitue
\begin{align*}
y=X_1\beta_1+X_2\beta_2+\epsilon
\end{align*}
we get
\begin{align*}
\hat \beta_2 =  (X_2^T  M_1X_2)^{-1}(X_2^T M_1\epsilon)  
\end{align*}
Since $\epsilon ~ N(0, \sigma^2I)$, we obtain
\begin{align*}
\beta_2 \sim N(0, \sigma^2(X_2^TM_1X_2)^{-1})
\end{align*}
Thus, scaling by $1/\sigma^2$, we arrive at
\begin{align*}
\frac{1}{\sigma^2}X_2^T M_1\epsilon \sim N(0,X_2^T M_1 X_2 )
\end{align*}
Construct Wald test W
\begin{align*}
W = \frac{\hat \beta_2}{\sqrt{\hat Var(\hat \beta_2)}}
\end{align*}
W follows t distribution. We now show W test is equivalent to LM test. Consider $W^2$
\begin{align*}
W^2 & =\hat \beta^T (Var(\hat \beta))^{-1} \hat \beta = \frac{1}{\sigma^2}\hat \beta^T V \hat \beta = \frac{1}{\sigma^2} (V^{-1}S_2)^T V V^{-1} S_2 =\frac{1}{\sigma^2} S_2^T V^{-1}VV^{-1}S_2 \\ &=  \frac{1}{\sigma^2}S_2^T V^{-1}S_2\\
       &= LM
\end{align*}

\section{Generalized Regression Model}
\subsection{Generalized Least Squares (GLS)}
\subsubsection{Problem Setting and Motivation}

Recall the linear regression model:
\[
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{\epsilon}
\]
where:
\begin{itemize}
    \item $\mathbf{y}$ is $n\times 1$ vector of outcomes
    \item $\mathbf{X}$ is $n\times k$ matrix of regressors (full column rank)
    \item $\boldsymbol{\beta}$ is $k\times 1$ parameter vector
    \item $\mathbf{\epsilon}$ is $n\times 1$ error vector
\end{itemize}

The classical OLS assumptions include spherical errors: 
\[
\mathbb{E}[\mathbf{\epsilon}\mid\mathbf{X}] = \mathbf{0}, \quad \mathbb{E}[\mathbf{\epsilon \epsilon}'\mid\mathbf{X}] = \sigma^2\mathbf{I}_n.
\]

When the second assumption fails (heteroskedasticity and/or autocorrelation), OLS remains unbiased but is no longer efficient. The Generalized Least Squares (GLS) framework provides the BLUE under a known non-spherical covariance structure.

\subsubsection{Assumptions}

Assume:
\begin{enumerate}
    \item $\mathbb{E}[\mathbf{\epsilon}\mid\mathbf{X}] = \mathbf{0}$
    \item $\mathbb{E}[\mathbf{\epsilon \epsilon}'\mid\mathbf{X}] = \sigma^2\mathbf{\Omega}$, where $\mathbf{\Omega}$ is a known $n\times n$ symmetric positive definite matrix.
    \item $\mathbf{X}$ has full column rank $k$, and $\mathbf{X}'\mathbf{\Omega}^{-1}\mathbf{X}$ is invertible.
\end{enumerate}

\subsubsection{Derivation via Transformation}

Since $\mathbf{\Omega}$ is positive definite, there exists a nonsingular matrix $\mathbf{P}$ such that:
\[
\mathbf{P}\mathbf{\Omega}\mathbf{P}' = \mathbf{I}_n.
\]
One common choice is $\mathbf{P} = \mathbf{\Omega}^{-1/2}$ (the inverse of the Cholesky factor).

Transform the model by premultiplying by $\mathbf{P}$:
\[
\mathbf{P}\mathbf{y} = \mathbf{P}\mathbf{X}\boldsymbol{\beta} + \mathbf{P}\mathbf{u}.
\]
Define $\mathbf{y}^* = \mathbf{P}\mathbf{y}$, $\mathbf{X}^* = \mathbf{P}\mathbf{X}$, $\mathbf{u}^* = \mathbf{P}\mathbf{u}$. Then:
\[
\mathbb{E}[\mathbf{u}^*\mid\mathbf{X}] = \mathbf{P}\mathbb{E}[\mathbf{u}\mid\mathbf{X}] = \mathbf{0},
\]
\[
\mathbb{E}[\mathbf{u}^*\mathbf{u}^{*\prime}\mid\mathbf{X}] = \mathbf{P}(\sigma^2\mathbf{\Omega})\mathbf{P}' = \sigma^2\mathbf{I}_n.
\]

Applying OLS to the transformed model gives the GLS estimator:
\[
\hat{\boldsymbol{\beta}}_{\text{GLS}} = (\mathbf{X}^{*\prime}\mathbf{X}^*)^{-1}\mathbf{X}^{*\prime}\mathbf{y}^* = (\mathbf{X}'\mathbf{P}'\mathbf{P}\mathbf{X})^{-1}\mathbf{X}'\mathbf{P}'\mathbf{P}\mathbf{y}.
\]
Since $\mathbf{P}'\mathbf{P} = \mathbf{\Omega}^{-1}$, we obtain the canonical form:
\[
\boxed{\hat{\boldsymbol{\beta}}_{\text{GLS}} = (\mathbf{X}'\mathbf{\Omega}^{-1}\mathbf{X})^{-1}\mathbf{X}'\mathbf{\Omega}^{-1}\mathbf{y}}.
\]

\subsubsection{Properties}

Under assumptions 1-3, $\hat{\boldsymbol{\beta}}_{\text{GLS}}$ is the Best Linear Unbiased Estimator (BLUE) of $\boldsymbol{\beta}$.

Let $\tilde{\boldsymbol{\beta}} = \mathbf{A}\mathbf{y}$ be any linear unbiased estimator. Write $\mathbf{A} = (\mathbf{X}'\mathbf{\Omega}^{-1}\mathbf{X})^{-1}\mathbf{X}'\mathbf{\Omega}^{-1} + \mathbf{D}$. Unbiasedness requires $\mathbf{D}\mathbf{X} = \mathbf{0}$. Then:
\[
\operatorname{Var}(\tilde{\boldsymbol{\beta}}\mid\mathbf{X}) = \sigma^2\left[(\mathbf{X}'\mathbf{\Omega}^{-1}\mathbf{X})^{-1} + \mathbf{D}\mathbf{\Omega}\mathbf{D}'\right].
\]
Since $\mathbf{D}\mathbf{\Omega}\mathbf{D}'$ is positive semidefinite, $\hat{\boldsymbol{\beta}}_{\text{GLS}}$ has minimal variance.


The asymptotic distribution (under appropriate regularity conditions) is:
\[
\sqrt{n}(\hat{\boldsymbol{\beta}}_{\text{GLS}} - \boldsymbol{\beta}) \stackrel{d}{\to} N\left(\mathbf{0}, \sigma^2 \mathbf{Q}^{-1}\right),
\]
where $\mathbf{Q} = \text{plim}\, \frac{1}{n}\mathbf{X}'\mathbf{\Omega}^{-1}\mathbf{X}$.

\subsection{Feasible Generalized Least Squares (FGLS)}

\subsubsection{The Problem of Unknown $\mathbf{\Omega}$}

In practice, $\mathbf{\Omega}$ is unknown. Feasible GLS (FGLS) replaces $\mathbf{\Omega}$ with a consistent estimator $\hat{\mathbf{\Omega}}$.

\subsubsection{Two-Step Procedure}

\begin{enumerate}
    \item \textbf{First step:} Obtain initial OLS estimator $\hat{\boldsymbol{\beta}}_{\text{OLS}}$ and residuals $\hat{\mathbf{u}} = \mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}}_{\text{OLS}}$.
    \item \textbf{Estimate $\mathbf{\Omega}$:} Model $\mathbf{\Omega}$ as a function of parameters $\boldsymbol{\theta}$, e.g.:
    \begin{itemize}
        \item For heteroskedasticity: $\Omega_{ii} = h(\mathbf{z}_i'\boldsymbol{\theta})$, where $\mathbf{z}_i$ may include $\mathbf{x}_i$.
        \item For AR(1) errors: $\Omega_{ij} = \rho^{|i-j|}$.
    \end{itemize}
    Estimate $\boldsymbol{\theta}$ from residuals (e.g., regress $\log(\hat{u}_i^2)$ on $\mathbf{z}_i$ for multiplicative heteroskedasticity).
    \item \textbf{Second step:} Compute FGLS estimator:
    \[
    \boxed{\hat{\boldsymbol{\beta}}_{\text{FGLS}} = (\mathbf{X}'\hat{\mathbf{\Omega}}^{-1}\mathbf{X})^{-1}\mathbf{X}'\hat{\mathbf{\Omega}}^{-1}\mathbf{y}}.
    \]
\end{enumerate}

\subsubsection{Large-Sample Properties}

Under regularity conditions (consistency of $\hat{\mathbf{\Omega}}$ and finite moments):
\[
\sqrt{n}(\hat{\boldsymbol{\beta}}_{\text{FGLS}} - \boldsymbol{\beta}) \stackrel{d}{\to} N(\mathbf{0}, \sigma^2 \mathbf{Q}^{-1}),
\]
the same asymptotic distribution as GLS. Thus, FGLS is \textbf{asymptotically efficient}.

\textbf{Important:} In finite samples, FGLS may not outperform OLS if $\mathbf{\Omega}$ is poorly estimated. Iterated FGLS (updating residuals and $\hat{\mathbf{\Omega}}$ until convergence) is sometimes used.

\subsection{Weighted Least Squares (WLS)}

\subsubsection{Special Case of GLS}

WLS arises when $\mathbf{\Omega}$ is diagonal (heteroskedasticity with no autocorrelation):
\[
\mathbf{\Omega} = \operatorname{diag}(\omega_1, \omega_2, \dots, \omega_n), \quad \omega_i > 0.
\]
Then $\mathbf{\Omega}^{-1} = \operatorname{diag}(\omega_1^{-1}, \dots, \omega_n^{-1})$.

\subsubsection{Derivation}

The GLS estimator simplifies to:
\[
\hat{\boldsymbol{\beta}}_{\text{WLS}} = \left(\sum_{i=1}^n \omega_i^{-1}\mathbf{x}_i\mathbf{x}_i'\right)^{-1} \left(\sum_{i=1}^n \omega_i^{-1}\mathbf{x}_i y_i\right).
\]
Equivalently, $\hat{\boldsymbol{\beta}}_{\text{WLS}}$ minimizes the weighted sum of squares:
\[
\boxed{\sum_{i=1}^n \omega_i^{-1}(y_i - \mathbf{x}_i'\boldsymbol{\beta})^2}.
\]
If we define weights $w_i = \omega_i^{-1}$, the estimator is called Weighted Least Squares.

\subsubsection{Common Weight Specifications}

\begin{itemize}
    \item \textbf{Known weights:} From survey data where $\omega_i$ reflects sampling probabilities.
    \item \textbf{Variance proportional to a known function:} Suppose $\operatorname{Var}(u_i\mid\mathbf{x}_i) = \sigma^2 v(\mathbf{x}_i)$. Then $\omega_i = v(\mathbf{x}_i)$.
    \item \textbf{Estimated weights:} A two-step FGLS procedure where:
    \begin{enumerate}
        \item Regress $y$ on $\mathbf{X}$ by OLS, get residuals $\hat{u}_i$.
        \item Regress $\hat{u}_i^2$ (or $\log \hat{u}_i^2$) on functions of $\mathbf{x}_i$ to estimate $v(\mathbf{x}_i)$.
        \item Use $\hat{\omega}_i = \hat{v}(\mathbf{x}_i)$ in WLS.
    \end{enumerate}
\end{itemize}

\subsection{Relationships and Comparison}

\begin{table}[h!]
\centering
\begin{tabular}{lccc}

Method & $\mathbf{\Omega}$ known? & Structure & Efficiency \\

OLS & $\mathbf{\Omega}=\mathbf{I}$ & Spherical errors & Inefficient if $\mathbf{\Omega}\neq\mathbf{I}$ \\
WLS & Diagonal known/estimated & Heteroskedastic only & Efficient within diagonal class \\
GLS & Known & General & BLUE \\
FGLS & Estimated & General & Asymptotically efficient \\

\end{tabular}
\caption{Comparison of Least Squares Methods}
\end{table}

\subsubsection{Key Theorems}

{\bf Equivalence of GLS and OLS on transformed data}\\
GLS is numerically equivalent to OLS on data transformed by $\mathbf{P} = \mathbf{\Omega}^{-1/2}$.

\noindent{\bf Invariance to choice of $\mathbf{P}$}\\
If $\mathbf{P}_1$ and $\mathbf{P}_2$ satisfy $\mathbf{P}_i\mathbf{\Omega}\mathbf{P}_i' = \mathbf{I}$, the GLS estimator is unchanged.


\noindent{\bf Asymptotic equivalence of FGLS and GLS}\\
If $\hat{\mathbf{\Omega}} \stackrel{p}{\to} \mathbf{\Omega}$ and certain moment conditions hold, $\hat{\boldsymbol{\beta}}_{\text{FGLS}}$ has the same asymptotic distribution as $\hat{\boldsymbol{\beta}}_{\text{GLS}}$.


\subsubsection{Empirical Considerations}

\subsection{Testing for Heteroskedasticity/Autocorrelation}
Before applying FGLS, test OLS residuals:
\begin{itemize}
    \item Breusch-Pagan / White test for heteroskedasticity
    \item Durbin-Watson / Breusch-Godfrey test for autocorrelation
\end{itemize}

\subsection{Robust Inference}
Even with FGLS, it is common to report \textbf{heteroskedasticity-robust standard errors} (White, 1980) because:
\begin{enumerate}
    \item The $\mathbf{\Omega}$ model may be misspecified.
    \item Robust standard errors provide valid inference under weaker conditions.
\end{enumerate}

The robust asymptotic variance estimator for FGLS is:
\[
\widehat{\operatorname{Var}}_{\text{robust}}(\hat{\boldsymbol{\beta}}_{\text{FGLS}}) = (\mathbf{X}'\hat{\mathbf{\Omega}}^{-1}\mathbf{X})^{-1} 
\left(\sum_{i=1}^n \hat{w}_i^2 \hat{u}_i^2 \mathbf{x}_i\mathbf{x}_i'\right)
(\mathbf{X}'\hat{\mathbf{\Omega}}^{-1}\mathbf{X})^{-1},
\]
where $\hat{w}_i$ are the FGLS weights and $\hat{u}_i$ the FGLS residuals.

\subsection*{Summary}
\begin{itemize}
    \item \textbf{GLS} is the theoretical benchmark with known $\mathbf{\Omega}$.
    \item \textbf{FGLS} is the feasible version using estimated $\hat{\mathbf{\Omega}}$; asymptotically equivalent to GLS.
    \item \textbf{WLS} is a special case for diagonal $\mathbf{\Omega}$ (pure heteroskedasticity).
    \item In practice, FGLS is commonly implemented with:
    \begin{enumerate}
        \item Model specification for the error structure
        \item Two-step estimation
        \item Robust inference to guard against misspecification
    \end{enumerate}
\end{itemize}
\section{Panel Data Model}
We can view panel data as a "two dimensional" data set in which the sample does not only come from different individuals, but also same individual across different time point.  We can write the regression model as
\begin{align*}
y_{it} = \alpha_{it} + \sum_k x_{itk} \beta_{itk} + u_{it}
\end{align*}
where $1<i<N$, $1<t<T$, and $1<k<K$. The equation has total sample size of $NT$ with total number of parameter $NT(K+1)$, therefore it is not estimable. So we will make the following few assumptions\\
\begin{tabular}{c |c | c | c | c}
&$\alpha_{it} = \alpha_{is}$ & $\alpha_{it} = \alpha_{jt}$ & $\beta_{it} = \beta_{is}$ & $\beta_{itk} = \beta_{jtk}$\\ 
\hline
Pooled&yes&yes&yes&yes\\
Fixed Effect&yes&no&yes&yes\\
Unrestricted&yes&no&yes&no\\
\end{tabular}
\subsection{The unrestriced model}
\begin{align*}
y_{it}=\alpha_i + \sum_k x_{itk}\beta_{ik} + u_{it}
\end{align*}
The above equation can be written in matrix form:\\
for $i = 1$,
\begin{align*}
 \left( \begin{array} { c  } 
                   y_{11}  \\
                   y_{12}  \\
                   ... \\
                   y_{1T} \\
           \end{array} \right)
       = \left( \begin{array} { c c c c c } 
                 1 &  x_{111} & x_{112} & ... & x_{11K} \\
                 1 &  x_{121} & x_{122} & ... & x_{12K}\\
                 1 &  ... & ... & ...&...\\
                 1 &  x_{1T1} & x_{1T2} & ... &x_{1TK}\\
           \end{array} \right)
            \left( \begin{array} { c } 
                   \alpha_1 \\
                   \beta_{11}  \\
                   \beta_{12}  \\
                   ... \\
                   \beta_{1K} \\
           \end{array} \right)
               +
            \left( \begin{array} { c  } 
                   u_{11}  \\
                   u_{12}  \\
                   ... \\
                   u_{1T} \\
           \end{array} \right)
\end{align*}
which we can also write as
for $i = 2$,
\begin{align*}
 \left( \begin{array} { c  } 
                   y_{21}  \\
                   y_{22}  \\
                   ... \\
                   y_{2T} \\
           \end{array} \right)
       = \left( \begin{array} { c c c c c } 
                 1 &  x_{211} & x_{212} & ... & x_{21K} \\
                 1 &  x_{221} & x_{222} & ... & x_{22K}\\
                 1 &  ... & ... & ...&...\\
                 1 &  x_{2T1} & x_{2T2} & ... &x_{2TK}\\
           \end{array} \right)
            \left( \begin{array} { c } 
                   \alpha_2 \\
                   \beta_{21}  \\
                   \beta_{22}  \\
                   ... \\
                   \beta_{2K} \\
           \end{array} \right)
               +
            \left( \begin{array} { c  } 
                   u_{21}  \\
                   u_{22}  \\
                   ... \\
                   u_{2T} \\
           \end{array} \right)
\end{align*}
So for each i, we can write
\begin{align*}
Y_i = 1_T \alpha_i+ X_i \beta_i + U_i
\end{align*}
where $Y_i = (y_{i1}, y_{i2}, ...,y_{iT})^T$, $1_T$ is a one vector of length T, $X_i$ is $K \times T $ matrix, $\beta_i = (\beta_{i1}, \beta_{i2}, ...,\beta_{iK})^T$, and $U_i = (u_{i1}, u_{i2}, ...,u_{iT})^T$.

If we consolidate equation set for all the value of i
\begin{align*}
 \left( \begin{array} { c  } 
                   Y_{1}  \\
                   Y_{2}  \\
                   ... \\
                   Y_{N} \\
           \end{array} \right)
       = \left( \begin{array} { c c c c c } 
                 1_{T} & 0 & 0 & ... & 0\\
                 0 & 1_{T} & 0 & ...& 0\\
                 0 & 0 & 1_{T} & ...& 0\\
                 0 &  0 & 0 & ... &1_{NT}\\
           \end{array} \right)
            \left( \begin{array} { c } 
                   \alpha_{1}  \\
                   \alpha_{2}  \\
                   ... \\
                   \alpha_{N} \\
           \end{array} \right) +
\left( \begin{array} { c c c c c } 
                 X_{1} & 0 & 0 & ... & 0\\
                 0 & X_{2} & 0 & ...& 0\\
                 0 & 0 & X_{3} & ...& 0\\
                 0 &  0 & 0 & ... &X_{N}\\
           \end{array} \right)
            \left( \begin{array} { c } 
                   \beta_{1}  \\
                   \beta_{2}  \\
                   ... \\
                   \beta_{N} \\
           \end{array} \right)
               +
            \left( \begin{array} { c  } 
                   U_{1}  \\
                   U_{2}  \\
                   ... \\
                   U_{N} \\
           \end{array} \right)
\end{align*}
To solve for $\beta_i$, we can use the strategy of partition regression. $\beta_i$ is the solution of the the regression
\begin{align*}
MY_i= MX_i\beta + U
\end{align*}
where 
\begin{align*}
M & = I -\frac{1}{T}1_T 1_T^{'} \\
MY_i & = y_{it}-\bar y_{i.} \\
MX_i & = x_{it}-\bar x_{i.} \\
\end{align*}
Here to avoid duplicate notation, we denote the transposed matrix using $'$. The estimate of $\beta$ is
\begin{align*}
\hat \beta_i = ((MX_i)^T(MX_i))^{-1}((MX_i)^T(MY_i))=  W_{xx,i}^{-1}W_{xy,i}
\end{align*}
where 
\begin{align*}
W_{xy,i} &= \sum_t^T(x_{it}-\bar x_{i.})(y_{it}-\bar y_{i.}) \\
W_{xx,i} &= \sum_t^T(x_{it}-\bar x_{i.})(x_{it}-\bar x_{i.})^T
\end{align*}
\subsection{The pooled model}
\begin{align*}
y_{it}=\alpha + \sum_k x_{itk}\beta_{k} + u_{it}
\end{align*}
The above equation can be written in matrix form:\\
for $i = 1$,
\begin{align*}
 \left( \begin{array} { c  } 
                   y_{11}  \\
                   y_{12}  \\
                   ... \\
                   y_{1T} \\
                   y_{21} \\
                   y_{22} \\
                   ... \\
                   y_{2T}\\
                   ...\\
                   y_{NT}\\
           \end{array} \right)
       = \left( \begin{array} { c c c c c } 
                 1 &  x_{111} & x_{112} & ... & x_{11K} \\
                 1 &  x_{121} & x_{122} & ... & x_{12K}\\
                 1 &  ... & ... & ...&...\\
                 1 &  x_{1T1} & x_{1T2} & ... &x_{1TK}\\
           	 1 &  x_{211} & x_{212} & ... & x_{21K} \\
                 1 &  x_{221} & x_{222} & ... & x_{22K}\\
                 1 &  ... & ... & ...&...\\
                 1 &  x_{2T1} & x_{2T2} & ... &x_{2TK}\\
                 1 &  ... & ... & ...&...\\
                 1 &  x_{NT1} & x_{NT2} & ... &x_{NTK}\\
           \end{array} \right)
            \left( \begin{array} { c } 
                   \alpha \\
                   \beta_{1}  \\
                   \beta_{2}  \\
                   ... \\
                   \beta_{K} \\
           \end{array} \right)
               +
            \left( \begin{array} { c  } 
                   u_{11}  \\
                   u_{12}  \\
                   ... \\
                   u_{1T} \\
   		   u_{21}  \\
                   u_{22}  \\
                   ... \\
                   u_{2T} \\
                   ...\\
                   u_{NT}\\
           \end{array} \right)
\end{align*}
Similarly, using the solution of $\beta$ from Eq.\ref{eq:beta_partition}, the estimated $\beta$ can be written as
\begin{align*}
M = I - \frac{1}{NT}1_{NT} 1_{NT}^{'}
\end{align*}
\begin{align*}
\hat \beta & = ((MX)^T(MX))^{-1}((MX)^T(MY_i)) \\
               & =(\sum_i^N\sum_t^T(x_{it}-\bar x_{..})(x_{it}- \bar x_{..})^T)^{-1}\sum_i^N\sum_t^T(x_{it}-\bar x_{..})(y_{it}- \bar y_{..}) \\
               & = T_{xx}^{-1}T_{xy} \\
\end{align*}
where 
\begin{align*}
T_{xy} &= \sum_i^N\sum_t^T(x_{it}-\bar x_{..})(y_{it}- \bar y_{..}) \\
T_{xx} &= \sum_i^N\sum_t^T(x_{it}-\bar x_{..})(x_{it}- \bar x_{..})^T\\
T_{yy} &= \sum_i^N\sum_t^T(y_{it}-\bar y)^2
\end{align*}
We call $T_{xx}$, $T_{yy}$ and $T_{xy}$ the total sum square of x, total sum square of y, and total sum of cross product.
The sum of square error for the pooled model is
\begin{align*}
SSE_{pooled} = T_{yy} - T^{'}_{xy} T^{-1}_{xx} T_{xy} 
\end{align*}
with N-1-K degrees of freedom.
\subsection{The fixed effect model}
\subsubsection{Model formulation and estimator}
\begin{align*}
y_{it}=\alpha_i + \sum_k x_{itk}\beta_{k} + u_{it}
\end{align*}
The above equation can be written in matrix form:\\
\begin{align*}
 \left( \begin{array} { c  } 
                   y_{11}  \\
                   y_{12}  \\
                   ... \\
                   y_{1T} \\
                   y_{21} \\
                   y_{22} \\
                   ... \\
                   y_{2T}\\
                   ...\\
                   y_{NT}\\
           \end{array} \right)
       = \left( \begin{array} { c c c c c } 
                 1_{T} & 0 & 0 & ... & 0\\
                 0 & 1_{T} & 0 & ...& 0\\
                 0 & 0 & 1_{T} & ...& 0\\
                 0 &  0 & 0 & ... &1_{T}\\
           \end{array} \right)
\left( \begin{array} { c } 
                  \alpha_{1}  \\
                  \alpha_{2}  \\
                  ... \\
                  \alpha_{N} \\
           \end{array} \right) \\
+
\left( \begin{array} {  c c c c } 
                   x_{111} & x_{112} & ... & x_{11K} \\
                  x_{121} & x_{122} & ... & x_{12K}\\
                  ... & ... & ...&...\\
                   x_{1T1} & x_{1T2} & ... &x_{1TK}\\
           	   x_{211} & x_{212} & ... & x_{21K} \\
                   x_{221} & x_{222} & ... & x_{22K}\\
                   ... & ... & ...&...\\
                   x_{2T1} & x_{2T2} & ... &x_{2TK}\\
                   ... & ... & ...&...\\
                  x_{NT1} & x_{NT2} & ... &x_{NTK}\\
           \end{array} \right)
            \left( \begin{array} { c } 
                   \beta_{1}  \\
                   \beta_{2}  \\
                   ... \\
                   \beta_{K} \\
           \end{array} \right)
               +
            \left( \begin{array} { c  } 
                   u_{11}  \\
                   u_{12}  \\
                   ... \\
                   u_{1T} \\
   		   u_{21}  \\
                   u_{22}  \\
                   ... \\
                   u_{2T} \\
                   ...\\
                   u_{NT}\\
           \end{array} \right)
\end{align*}
Where $1_T$ is a $1\times T$ vector. Similarly, using the solution of $\beta$ from Eq.\ref{eq:beta_partition}, the M matrix is
\begin{align*}\label{eq:remove_mean_matrix}
M=I -\frac{1}{T}\left( \begin{array} { c c c c c } 
                 1_{T\times T} & 0 & 0 & ... & 0\\
                 0 & 1_{T\times T} & 0 & ...& 0\\
                 0 & 0 & 1_{T \times T} & ...& 0\\
                 0 &  0 & 0 & ... &1_{T \times T}\\
           \end{array} \right)
\end{align*}
Define  
\begin{align*}
\tilde x_{itk} = (MX)_{itk} =x_{itk} - \bar x_{i.k}
\end{align*}
which is an NT x K matrix,
\begin{align*}
\tilde y_{it} = (MY)_{it} = y_{it} - \bar y_{i.}
\end{align*}
which is an NT x 1 vector.
The estimated $\beta$ can be written as
\begin{align*}
\hat \beta  = ((MX)^{'}(MX))^{-1}((MX)^T(MY_i)) 
\end{align*}
and
\begin{align*}
((MX)^{'}(MX))_{kl} = \sum_i \sum_t \tilde x_{itk}  \tilde x_{itl} \\
((MX)^{'}(MY))_k = \sum_i \sum_t x_{itk} \tilde y_{it} \\
\end{align*}
Let $\tilde X_{it} = (x_{it1} - \bar x_{i.1}, x_{it2} - \bar x_{i.2},...,x_{itk} - \bar x_{i.k})$, $\tilde y_{it} = y_{it} - \bar y_{i.}$
Then 
\begin{align*}
((MX)^{'}(MX)) &= \sum_i \sum_t \tilde X_{it} \tilde X_{it}^{'}\\
((MX)^{'}(MY)) &= \sum_i \sum_t \tilde X_{it} \tilde y_{it} \\
\end{align*}
\begin{align*}
\hat \beta & =[\sum_i^N\sum_t^T\tilde X_{it}\tilde X_{it}^{'}]^{-1}[\sum_i^N\sum_t^T\tilde X_{it} \tilde y_{it}] \\
               & = W_{xx}^{-1}W_{xy}\\
\end{align*}
where 
\begin{align*}
W_{xy} &= \sum_i^N\sum_t^T \tilde X_{it} \tilde y_{it} \\
W_{xx} &= \sum_i^N\sum_t^T\tilde X_{it}\tilde X_{it}^T\\
W_{yy} &= \sum_i^N\sum_t^T\tilde y_{it}^2\\
\end{align*}
We call $W_{xx}$, $W_{yy}$ and $W_{xy}$ the within-groups sum square of x, the within-groups sum square of y, and within-groups of cross product. The name within-groups means they utilized the variation within group i. The sum of square error is
\begin{align*}
SSE_{fix} = W_{yy} - W^{'}_{xy}W^{-1}_{xx}W^{xy}
\end{align*}
with NT - N - K degrees of freedom.\\
If we remind ourselves of the concept of partial regression, we let
\begin{align*}
Y^{*} = MY
X^{*} = MX
U^{*} = MU
\end{align*}
then the fix effect model can also be written as
\begin{align*}
Y^{*} = X^{*}\beta + U^{*}
\end{align*}
or 
\begin{align*}
y_{it} - \bar y_{i.} = (X_{it} - \bar X_{i.}) \beta +  (u_{it} - \bar u_{i.})
\end{align*} 
\subsubsection {Properties of Fixed effect estimator}
1) Unbiasedness\\
\begin{align*}
\hat \beta & =  [\sum_i^N\sum_t^T\tilde X_{it}X_{it}^{'}]^{-1}[\sum_i^N\sum_t^T(\tilde X_{it})( \tilde y_{it})] \\
               & =  [\sum_i^N\sum_t^T\tilde X_{it} X_{it}^{'}]^{-1}[\sum_i^N\sum_t^T(\tilde X_{it})( \tilde X^{'}_{it}\beta + \tilde u_{it})] \\
              & =  [\sum_i^N\sum_t^T\tilde X_{it} X_{it}^{'}]^{-1}[\sum_i^N\sum_t^T(\tilde X_{it} \tilde X^{'}_{it}\beta + \tilde X_{it}\tilde u_{it})]\\
              & = [\sum_i^N\sum_t^T\tilde X_{it} X_{it}^{'}]^{-1}[\sum_i^N\sum_t^T \tilde X_{it} \tilde X^{'}_{it}]\beta 
                   + [\sum_i^N\sum_t^T\tilde X_{it} X_{it}^{'}]^{-1} [\sum_i^N \sum_t^T\tilde X_{it}\tilde u_{it}]
\end{align*}
So 
\begin{align*}
E[\hat \beta] = \beta + E[\sum_i^N\sum_t^T\tilde X_{it} X_{it}^{'}]^{-1} [\sum_i^N \sum_t^T \tilde X_{it}\tilde u_{it}]
\end{align*}
As $u_{it}$ is uncorrelated with $x_{it}$, it is easy to prove the 2nd term above is zero. Therefore, the within group estimator is unbiased.\\
2) Consistency\\
To show consistency, we need to prove
\begin{align*}
[\sum_i^N\sum_t^T\tilde X_{it} X_{it}^{'}]^{-1} [\sum_i^N \sum_t^T\tilde X_{it}\tilde u_{it}] 
= [\frac{1}{NT}\sum_i^N\sum_t^T\tilde X_{it} X_{it}^{'}]^{-1} [\frac{1}{NT}\sum_i^N \sum_t^T\tilde X_{it}\tilde u_{it}]  = 0 
\end{align*}
Similar to our previous consistency proof, the above converges to zero when 1) either T or N goes to infinity, 2) $\tilde x_{it}$ and $\tilde u_{it}$ are not correlated, 3) $x_{it}$ and $u_{it} $ has finite second moment.\\
\subsubsection {Connections between fixed effect model estimator and pooled model estimator when T is large}
To see the connection between fixed effect estimator and pooled model estimator, we first introduce between-groups estimator, which allows us to easily understand the relationship among differect estimators.\\
We define the between-groups sum of square is
\begin{align*}
B_{xx} & = \sum_i^NT(\bar x_{i.}-\bar x_{...})(\bar x_{i.}-\bar x_{...})^{'} \\
B_{yy} &= \sum_i^NT(\bar y_{i.}-\bar y_{...})(\bar y_{i.}-\bar y_{...})^{'}\\
B_{xy} &= \sum_i^NT(\bar x_{i.}-\bar x_{...})(\bar y_{i.}-\bar y_{...})\\
\end{align*}
It is easy to show that 
\begin{align*}
T_{xx} &= W_{xx} + B_{xx} \\
T_{yy} &= W_{yy} + B_{yy} \\
T_{xy} &= W_{xy} + B_{xy} \\
\end{align*}
And similarly we can define between-groups estimator
\begin{align*}
\beta_{between} = \frac{B_{xy}}{B_{xx}}
\end{align*}
We rewrite the  estimator of $\beta$ for the pooled model 
\begin{align*}
\beta_{pooled} & = \frac{T_{xy}}{T_{xx}} \\
                      & = \frac{B_{xy}}{T_{xx}} + \frac{W_{xy}}{T_{xx}}\\
                      & = \frac{B_{xx}}{T_{xx}}\frac{B_{xy}}{B_{xx}}  +  \frac{W_{xx}}{T_{xx}} \frac{W_{xy}}{W_{xx}} \\
\end{align*}
because
\begin{align*}
T_{xx} = W_{xx} + B_{xx}
\end{align*}
So if we define 
\begin{align*}
\omega = \frac{W_{xx}}{T_{xx}}
\end{align*}
then 
\begin{align*}
\beta_{pooled} & = \omega \frac{W_{xy}}{W_{xx}}+ (1 - \omega) \frac {B_{xy}}{B_{xx}} 
\end{align*}
When $T \rightarrow \infty$, how do $W_{xx}$ and $B_{xx}$ behave? remember
\begin{align*}
B_{xx} & = \sum_i^NT(\bar x_{i.}-\bar x_{...})(\bar x_{i.}-\bar x_{...})^{'} \\
W_{xx} &= \sum_i^N\sum_t^T(x_{it}-\bar x_{i.})(x_{it}- \bar x_{i.})^T\\
\end{align*} 
When T increase, $\bar x_{i.}$ would move close to $E[x_{i.}]$. so its variance would decrease, so $B_{xx}$ grows sub-linealy. In practice, most of the panel data noise comes from transitory noise. so $W_{xx}$ grows faster than $B_{xx}$ when T is large. Then $\omega \rightarrow 1$. So in large T, the pooled estimator converges to fix effect estimator.\\
\subsubsection{Limitations of fixed effect model}
1)Time-invariant variables are dropped\\
For a given individual i and regressor k, and any different time t and s($t \neq s$), if $x_{itk}=x_{isk}$, this x variable will be dropped during estimation. Because this condition leads to $x_{it} = \bar x_{i.}$\\  
2)Loss degree of freedom\\
In fixed effect, each individual adds up one parameter, so total N individuals need N parameters. This can cause problem in short panel.
\subsubsection{F test for fixed effect model}
\begin{align*}
F & = \frac{(SSE_{pooled}-SSE_{fix})/((NT-1-K)-(NT-N-K))}{SSE_{fix}/(NT-N-K)} \\
&= \frac{(SSE_{pooled}-SSE_{fix})/(N-1)}{SSE_{fix}/(NT-N-K)}
\end{align*}
\subsection{The random effect model}
\subsubsection{Model formulation and estimator}
In fixed effect model, we treat individual mean $\alpha_i$ is a constant. In random effect model, we treat $\alpha_i$ as a random variable. We write our random effect model as
\begin{align*}
y_{it} = \sum_k x_{itk}\beta_k + \alpha_i + u_{it}
\end{align*}
Where $\alpha_i \in Normal(0, \sigma_{\alpha}^2)$, $u_{it} \in Normal(0, \sigma^2_{u})$, $E(\alpha_i u_{it}) = 0$. 
Define
\begin{align*}
v_{it} = \alpha_i + u_{it}
\end{align*}
The variance of $v_{it}$ for fixed i is 
\begin{align*}
E(v_{it}v_{is}) = E(\alpha_i + u_{it})(\alpha_i + u_{is}) = \sigma^2_{\alpha} + \delta_{ts} \sigma^2_{u}
\end{align*}
The last term is non-zero only when t = s.
Thus, the covariance matrix for individual i is 
\begin{align*}
V_i = \left( \begin{array} {  c c c c } 
                   \sigma^2_{\alpha} + \sigma^2_{u} &  \sigma^2_{\alpha}  & ... &  \sigma^2_{\alpha}  \\
                   \sigma^2_{\alpha} & \sigma^2_{\alpha} + \sigma^2_{u} & ... & \sigma^2_{\alpha} \\
                  ... & ... & ...&...\\
                    \sigma^2_{\alpha} &  \sigma^2_{\alpha} & ... &\sigma^2_{\alpha} + \sigma^2_{u} \\
           \end{array} \right)
\end{align*}
where $V_i$ is  $T \times T$ matrix. Stack together for all individuals, the whole covariance matrix is
\begin{align*}
V =  \left( \begin{array} {  c c c c } 
                   V_1 &  0  & ... &  0  \\
                   0 & V_2& ... & 0 \\
                  ... & ... & ...&...\\
                  0 &  0 & ... & V_N\\
           \end{array} \right)
\end{align*}
Based on the solution of GLS, we define
\begin{align*}
V_i^{-1/2} = \frac{1}{\sigma_u}[I-\frac{\theta}{T} 1_T 1^{'}_T]
\end{align*}
where
\begin{align*}
\theta = 1 - \frac{\sigma_u}{\sqrt{\sigma^2_u + T \sigma^2_{\alpha}}}
\end{align*}
and the transformation of $X_i$ and $y_i$
\begin{align*}
\tilde{y_i} & = V_i^{-1/2}y_i = y_i - \theta \bar y_i\\
\tilde{X_{i}} & = V_i^{-1/2}X_{i} = X_i - \theta \bar X_i\\
\end{align*}
where $y_i$ is $T\times 1$ vector, and $X_{i}$ is $T \times K$ matrix.  The estimator of $\beta$ becomes
\begin{align*}
\hat \beta_{RE} = (\tilde X^{'}V^{-1}\tilde X)^{-1} (\tilde X^{'}V^{-1}\tilde Y)
\end{align*}
With a little derivation, we can prove $\beta_{RE}$ is a combination of estimator of pooled model and fixed effect model:
\begin{align*}
\hat \beta_{RE} = (1-\omega)\hat \beta_{pooled} + \omega \hat \beta_{FE}
\end{align*}
where
\begin{align*}
\omega = \frac{T\sigma^2_{\alpha}}{T\sigma^2_{\alpha} + \sigma^2_u}
\end{align*}
\subsubsection{Connections to estimator of pooled model and fixed effect model}
1) when $\sigma_{\alpha} >> \sigma_u$, then $\omega$ $\rightarrow$ 1, $\theta$ $\rightarrow$ 1,  this leads to fixed-effect model. In fixed effect model\\
a. Large $\sigma_{\alpha}$ means large viariation of $\alpha$ for different i, in other words, $\alpha_i$ is very different.\\
b. Since $Y_it$ for fixed i is center around $\alpha_i$, this means centers of $Y_i$ given different i are far apart.\\
c. The smaller $\sigma_u$ compared to $\sigma_{\alpha}$ means the variation of $Y_it$ is small. Therefore, the probability density functions of $Y_i$ for different i barely overlap, and their tails almost do not touch each other at all.\\
2) when $\sigma_{\alpha} << \sigma_u$, then $\omega$ $\rightarrow$ 0, $\theta$ $\rightarrow$ 0, this leads to pooled model. In pooled model,\\
a. Small $\sigma_{\alpha}$ means $\alpha_i$s are almost identitical .\\
b. Since $Y_it$ for fixed i is center around $\alpha_i$, this means centers of $Y_i$ given different i are very close to each other.\\
c. The larger $\sigma_u$ compared to $\sigma_{\alpha}$ means the variation of $Y_it$ is large. Therefore, the probability density functions of $Y_i$ for different i are completely overlapping. The difference between different individual i is hardly visible. So it is unnecessary to model the individual mean. \\
\subsubsection{Estimation of $\sigma^2_{u}$ and $\sigma^2_{\alpha}$}
Estimation of $\sigma^2_{u}$ can be based on the regression formula
\begin{align*}
y_{it} - \bar y_{i.} = (X_{it} - \bar X_{i.}) \beta+  (u_{it} - \bar u_{i.})
\end{align*}
and $\sigma^2_u$ can be estimated using sum of square error and its degrees of freedom.
\begin{align*}
\hat \sigma^2_u = \frac{\sum_{i=1}^N \sum_{t=1}^T [(y_{it} - \bar y_i) - \hat \beta_{fix}(x_{it}-\bar x_i)]}{N(T-1) -K}
\end{align*}
$\sigma^2_{\alpha}$ can be estimated by first taking the average of random effect model.
\begin{align*}
\bar y_{i} = \mu + \bar X_i \beta + \alpha_i + \bar u_{i}
\end{align*}
Then because $\alpha$ and $u_{i}$ are independent, 
\begin{align*}
Var(\bar y_i - \mu - \bar X_i \beta) = Var(\alpha) + Var(\bar u_{i}) = Var(\alpha) + \frac{1}{T}Var(u) 
\end{align*}
Then
\begin{align*}
\sigma^2_{\alpha} = \frac{\sum_{i=1}^N(\bar y_i - \hat \mu - X_i \hat \beta)^2}{N-(K+1)} -\frac{1}{T}\hat \sigma^2_u
\end{align*}
\subsubsection{Coping with the Limitation of Random Model}
The random model, like linear model in general, assumes $\alpha$ does not correlate with variable $X$. Mundlak intruduced auxiliary regression where we write
\begin{align*}
\alpha_i = \sum_k \bar x_{i.k} a_k +\omega_i
\end{align*}
where $\omega_i \in Normal(0, \sigma^2_{\omega})$. So
\begin{align*}
y_{it} = \sum_k x_{itk}\beta_{k} + \sum_k \bar x_{i.k}a_k + \omega_i + u_{it}
\end{align*}
If we define the error term 
\begin{align*}
v_{it} = \omega_i + u_{it}
\end{align*}
The above equation can be written in matrix form:\\
\begin{align*}
 \left( \begin{array} { c  } 
                   y_{11}  \\
                   y_{12}  \\
                   ... \\
                   y_{1T} \\
                   y_{21} \\
                   y_{22} \\
                   ... \\
                   y_{2T}\\
                   ...\\
                   y_{NT}\\
           \end{array} \right)
       = \left( \begin{array} { c c c c c } 
                 1_{T} & 0 & 0 & ... & 0\\
                 0 & 1_{T} & 0 & ...& 0\\
                 0 & 0 & 1_{T} & ...& 0\\
                 0 &  0 & 0 & ... &1_{T}\\
           \end{array} \right)
\left( \begin{array} { c  c  c  c} 
                \bar x_{1.1} &  \bar x_{1.2}  & ... & \bar x_{1.K} \\
                \bar x_{2.1} &  \bar x_{2.2}  & ... & \bar x_{2.K} \\
                \bar x_{3.1} &  \bar x_{3.2}  & ... & \bar x_{3.K} \\  
                  ... & ... & ... & ...\\
                \bar x_{N.1} &  \bar x_{N.2}  & ... & \bar x_{N.K} \\ 
           \end{array} \right) 
\left( \begin{array} { c } 
                \bar a_{1} \\
                \bar a_{2} \\
                \bar a_{3} \\  
                  ... \\
                \bar a_{K} \\ 
           \end{array} \right) \\
+
\left( \begin{array} {  c c c c } 
                   x_{111} & x_{112} & ... & x_{11K} \\
                  x_{121} & x_{122} & ... & x_{12K}\\
                  ... & ... & ...&...\\
                   x_{1T1} & x_{1T2} & ... &x_{1TK}\\
           	   x_{211} & x_{212} & ... & x_{21K} \\
                   x_{221} & x_{222} & ... & x_{22K}\\
                   ... & ... & ...&...\\
                   x_{2T1} & x_{2T2} & ... &x_{2TK}\\
                   ... & ... & ...&...\\
                  x_{NT1} & x_{NT2} & ... &x_{NTK}\\
           \end{array} \right)
            \left( \begin{array} { c } 
                   \beta_{1}  \\
                   \beta_{2}  \\
                   ... \\
                   \beta_{K} \\
           \end{array} \right)
               +
            \left( \begin{array} { c  } 
                   v_{11}  \\
                   v_{12}  \\
                   ... \\
                   v_{1T} \\
   		   v_{21}  \\
                   v_{22}  \\
                   ... \\
                   v_{2T} \\
                   ...\\
                   v_{NT}\\
           \end{array} \right)
\end{align*}
Similarly to the original random model, the variance matrix of $v_{it}$ is
\begin{align*}
V_i = \left( \begin{array} {  c c c c } 
                   \sigma^2_{\omega} + \sigma^2_{u} &  \sigma^2_{\omega}  & ... &  \sigma^2_{\omega}  \\
                   \sigma^2_{\omega} & \sigma^2_{\omega} + \sigma^2_{u} & ... & \sigma^2_{\omega} \\
                  ... & ... & ...&...\\
                    \sigma^2_{\omega} &  \sigma^2_{\omega} & ... &\sigma^2_{\omega} + \sigma^2_{u} \\
           \end{array} \right)
\end{align*}
The way to estimator $\beta$ should be GLM, but what happens if we just run OLS on this model by assuming $v_{it}$ follows standard normail distribution? Remember the method of doing partial regression. Running OLS on this model is equivalently running the {\bf residual } of Y after regressing on $\bar X_{i.}$ on the residual of X after regressing on $\bar X_{i.}$. \\
Obviously, regressing $X_{it}$ on ${X_{i.}}$ leads the residual
\begin{align*}
X^{*}_{it} = X_{it} - \bar X_{i.}
\end{align*}
Where each term is a K by 1 vector
Let us regress $Y_{it}$ on ${\bar X_{i.}}$ and compute the residual. The regression is
\begin{align*}
y_{it} = \mu + \lambda \bar x_{i.} + \eta_{it}
\end{align*}
We realize that $\mu$ and $\bar x_{i.}$ are constant for a given i. So the fitted value on a constant is the mean of the $y_{it}$ with respect t.  We can write
\begin{align*}
\bar y_{i.} = \sum_k \beta_k \bar x_{i.k} + \sum_k \bar x_{i.k} a_k + \bar v_{i} = \sum_k (\beta_k + a_k) \bar x_{i.k} + \bar v_{i}
\end{align*}
We know $E[v_{i}]=0$. So when we regress y on $\bar x_{i.}$, the fitted value becomes
\begin{align*}
\hat y_{it} =  \sum_k (\beta_k + a_k) \bar x_{i.k} 
\end{align*}
Then the residual is
\begin{align*}
y_{it} - \hat y_{it} & =  \sum_k x_{ik}\beta_{k} + \sum_k \bar x_{i.k}a_k + \epsilon_{it}
                           - (\sum_k (\beta_k + a_k) \bar x_{i.k}) \\
                           & = \sum_k \beta_k(x_{ik} - \bar x_{i.k} ) + \epsilon_{it} - \bar \epsilon_{i.} \\
                           & = y_{it} - \bar y_{i.} \\
\end{align*}
So the OLS we try to run is regress $y_{it} -\bar y_{i.}$ on $x_{it} - \bar x_{i.}$, which is equivalent to fixed effect estimation.\\
\subsection{Fixed Effect Model Generalization}
We can generalize fixed effect model by including more individual specific variables which vary across different individuals and but do not vary over time. We can write the model as
\begin{align*}
y_{it} & = \mu + \alpha_i + z_{i1}\gamma_1 + z_{i2}\gamma_2 + ... + z_{ip} \gamma_p + \sum_k x_{itk} \beta_k +u_{it}\\ 
         & = \mu + \alpha_i + \sum_p z_{ip}\gamma_p +
\sum_k x_{itk} \beta_k +u_{it}
\end{align*}
\begin{align*}
 \left( \begin{array} { c  } 
                   y_{11}  \\
                   y_{12}  \\
                   ... \\
                   y_{1T} \\
                   y_{21} \\
                   y_{22} \\
                   ... \\
                   y_{2T}\\
                   ...\\
                   y_{NT}\\
           \end{array} \right)
       = \left( \begin{array} { c c c c c } 
                 1_{T} & 0 & 0 & ... & 0\\
                 0 & 1_{T} & 0 & ...& 0\\
                 0 & 0 & 1_{T} & ...& 0\\
                 0 &  0 & 0 & ... &1_{T}\\
           \end{array} \right)
\left( \begin{array} { c } 
                  \alpha_{1}  \\
                  \alpha_{2}  \\
                  ... \\
                  \alpha_{N} \\
           \end{array} \right) \\
+
 \left( \begin{array} { c c c c c } 
                 1_{T} & 0 & 0 & ... & 0\\
                 0 & 1_{T} & 0 & ...& 0\\
                 0 & 0 & 1_{T} & ...& 0\\
                 0 &  0 & 0 & ... &1_{T}\\
           \end{array} \right)
\left( \begin{array} { c c c c } 
                  z_{11} & z_{12} & ... & z_{1p}   \\
                  z_{21} & z_{22} & ... & z_{2p}  \\
                  ... & ... & ... & ...\\
                  z_{N1} & z_{N2} & ... & z_{Np} \\
           \end{array} \right) 
\left( \begin{array} { c } 
                  \gamma_{1}  \\
                  \gamma_{2}  \\
                  ... \\
                  \gamma_{N} \\
           \end{array} \right) \\
+
\left( \begin{array} {  c c c c } 
                   x_{111} & x_{112} & ... & x_{11K} \\
                  x_{121} & x_{122} & ... & x_{12K}\\
                  ... & ... & ...&...\\
                   x_{1T1} & x_{1T2} & ... &x_{1TK}\\
           	   x_{211} & x_{212} & ... & x_{21K} \\
                   x_{221} & x_{222} & ... & x_{22K}\\
                   ... & ... & ...&...\\
                   x_{2T1} & x_{2T2} & ... &x_{2TK}\\
                   ... & ... & ...&...\\
                  x_{NT1} & x_{NT2} & ... &x_{NTK}\\
           \end{array} \right)
            \left( \begin{array} { c } 
                   \beta_{1}  \\
                   \beta_{2}  \\
                   ... \\
                   \beta_{K} \\
           \end{array} \right)
               +
            \left( \begin{array} { c  } 
                   u_{11}  \\
                   u_{12}  \\
                   ... \\
                   u_{1T} \\
   		   u_{21}  \\
                   u_{22}  \\
                   ... \\
                   u_{2T} \\
                   ...\\
                   u_{NT}\\
           \end{array} \right)
\end{align*}
This model suffers multicollinearity so $\alpha$ and $\gamma$ are not both estimable. We can set
\begin{align*}
\alpha^{*}_i = \alpha_i + \sum_p z_{ip} \gamma_p
\end{align*}
Then the model is effectively the same as the original fixed effect model, so we can use the same trick of solving fixed effect model to estimate $\beta$. To estimate $\gamma$, we note
\begin{align*}
\bar y_{i.} - \sum \bar x_{i.k}\beta_k = \sum_p z_{ip}\gamma_p + \alpha_i + \bar u_i
\end{align*} 
Let $\epsilon_i = \alpha_i + \bar u_i$ and by minimizing $\sum_i \epsilon^2_i$, we obtain
\begin{align*}
\hat \gamma = [\sum^N_{i=1}(z_i - \bar z)(z_i - \bar z)^{*}]^{-1} \{ \sum^N_{i=1} (z_i - \bar z )[(\bar y_i - \bar y) - (\bar x_i - \bar x)^{'} \beta]\}
\end{align*}
{\bf Testing of $\sigma_{\alpha}$}
We define the null and alternative hypothesis\\
$H_0: \sigma^2_{\alpha} = 0$ \\
$H_1: \sigma^2_{\alpha} > 0$ \\
We construct LM test, which uses the score function(gradient of log-likelyhood) with respect to $\sigma^2_\alpha$ evaluated at $\sigma^2_\alpha=0$:
\begin{align*}
LM & = \frac{(\partial l/\partial\sigma^2_{\alpha})^2}{Var(\partial l/\partial\sigma^2_{\alpha})}|_{\sigma^2_{\alpha}}\\
& = \frac{nT}{2(T-1)}[\frac{[\sum_{i=1}^n(\bar e_i)^2]}{(\frac{1}{nT}\sum_{i=1}^{n}\sum_{t=1}^Te^2_{it})}-1]^2\\
\end{align*}
\section{Panel Data Time-Dependent Models}

\subsection{Introduction and Model Specification}

Panel data (or longitudinal data) consist of observations on $N$ cross-sectional units (individuals, firms, countries, etc.) over $T$ time periods. Time dependence in panel models arises when the error terms or dependent variables exhibit correlation across time for the same unit. A general linear dynamic panel model can be written as:

\[
y_{it} = \alpha y_{i,t-1} + \mathbf{x}_{it}'\boldsymbol{\beta} + \eta_i + \varepsilon_{it}, \quad i=1,\dots,N, \quad t=1,\dots,T,
\]
where:
\begin{itemize}
    \item $y_{it}$ is the dependent variable for unit $i$ at time $t$,
    \item $y_{i,t-1}$ is the lagged dependent variable (introducing \textbf{dynamics}),
    \item $\mathbf{x}_{it}$ is a $k \times 1$ vector of exogenous regressors,
    \item $\eta_i$ is the \textbf{unobserved individual effect} (fixed or random),
    \item $\varepsilon_{it}$ is the idiosyncratic error term.
\end{itemize}

\noindent Key features of time dependence:
\begin{enumerate}
    \item \textbf{State dependence}: Past outcomes affect current outcomes ($y_{i,t-1}$ term).
    \item \textbf{Serial correlation}: $\varepsilon_{it}$ may follow an AR(p) process.
    \item \textbf{Heterogeneity}: Individual-specific effects $\eta_i$ that may be correlated with regressors.
\end{enumerate}

\subsection{Challenges in Estimation}

\begin{itemize}
    \item \textbf{Dynamic panel bias}: The lagged dependent variable $y_{i,t-1}$ is correlated with the individual effect $\eta_i$, making standard estimators (OLS, fixed effects) inconsistent for fixed $T$ as $N \to \infty$.
    
    \item \textbf{Nickell bias}: The within (fixed effects) estimator is biased of order $O(1/T)$, which diminishes only when $T$ is large.
    
    \item \textbf{Initial conditions problem}: The process may not be stationary, and the initial observation $y_{i0}$ may be correlated with $\eta_i$.
\end{itemize}

\subsection{Estimation Methods}

\subsubsection{Generalized Method of Moments (GMM)}
The most common approach for dynamic panels with small $T$ and large $N$.

\textbf{First-difference transformation} to eliminate $\eta_i$:
\[
\Delta y_{it} = \alpha \Delta y_{i,t-1} + \Delta \mathbf{x}_{it}'\boldsymbol{\beta} + \Delta \varepsilon_{it}, \quad t=2,\dots,T.
\]
The problem: $\Delta y_{i,t-1}$ is correlated with $\Delta \varepsilon_{it}$.

\noindent \textbf{Arellano-Bond (1991) GMM}:
\begin{itemize}
    \item Use \textbf{lagged levels} of $y_{it}$ as instruments for $\Delta y_{i,t-1}$:
    \begin{itemize}
        \item For $\Delta y_{i2}$, instrument: $y_{i0}$
        \item For $\Delta y_{i3}$, instruments: $y_{i0}, y_{i1}$
        \item For $\Delta y_{iT}$, instruments: $y_{i0}, y_{i1}, \dots, y_{i,T-2}$
    \end{itemize}
    \item Moment conditions: $\mathbb{E}[y_{i,t-s} \Delta \varepsilon_{it}] = 0$ for $s \geq 2$, $t=2,\dots,T$.
    \item Efficient GMM estimator: 
    \[
    \hat{\boldsymbol{\theta}}_{\text{GMM}} = \left( \mathbf{Z}'\mathbf{X} \mathbf{W}_N \mathbf{X}'\mathbf{Z} \right)^{-1} \mathbf{Z}'\mathbf{X} \mathbf{W}_N \mathbf{Z}'\mathbf{y},
    \]
    where $\mathbf{Z}$ is the instrument matrix, $\mathbf{X}$ the regressor matrix, and $\mathbf{W}_N$ a weight matrix.
\end{itemize}

\noindent \textbf{Blundell-Bond (1998) System GMM}:
\begin{itemize}
    \item Combines equations in differences with equations in levels.
    \item Additional moment conditions: Use lagged differences as instruments for levels equations.
    \item More efficient when $\alpha$ is close to 1 or when series are persistent.
    \item Moment conditions:
    \begin{align*}
        &\mathbb{E}[y_{i,t-s} \Delta \varepsilon_{it}] = 0 \quad \text{(difference equations)} \\
        &\mathbb{E}[\Delta y_{i,t-s} (\eta_i + \varepsilon_{it})] = 0 \quad \text{(level equations)}
    \end{align*}
    for appropriate $s$.
\end{itemize}

\subsubsection{Maximum Likelihood Estimation (MLE)}
For known distribution of errors, MLE can be used:
\[
L(\boldsymbol{\theta}) = \prod_{i=1}^N f(y_{i1}, \dots, y_{iT} | \mathbf{x}_{i1}, \dots, \mathbf{x}_{iT}; \boldsymbol{\theta}).
\]
\begin{itemize}
    \item \textbf{Conditional MLE}: Treat initial conditions $y_{i0}$ as given.
    \item \textbf{Unconditional MLE}: Model the distribution of $y_{i0}$ (e.g., assume stationarity).
    \item Implementation requires specifying the joint distribution of $(\eta_i, \varepsilon_{i1}, \dots, \varepsilon_{iT})$.
\end{itemize}

\subsubsection{Long-T Panels: Bias-Corrected Estimators}
When $T$ is moderately large (e.g., $T > 10$):
\begin{itemize}
    \item \textbf{Bias-corrected fixed effects}: Correct the Nickell bias using analytical or bootstrap methods.
    \item \textbf{Least Squares Dummy Variable (LSDV)} with correction:
    \[
    \hat{\alpha}_{\text{corrected}} = \hat{\alpha}_{\text{LSDV}} + \frac{1+\hat{\alpha}_{\text{LSDV}}}{T-1}.
    \]
\end{itemize}

\subsection{Hypothesis Testing}

\subsubsection{Specification Tests}

\textbf{1. Serial Correlation Tests}:
\begin{itemize}
    \item \textbf{Arellano-Bond test for autocorrelation}:
    \begin{itemize}
        \item Test $H_0$: No autocorrelation in first-differenced errors at order $m$.
        \item Based on sample autocovariances of differenced residuals.
        \item Critical: Test for AR(1) in $\Delta \varepsilon_{it}$ (expected) and AR(2) (should be zero under null).
    \end{itemize}
    
    \item \textbf{Breusch-Godfrey test for panel data}: LM test for serial correlation.
\end{itemize}

\textbf{2. Overidentification/Specification Tests}:
\begin{itemize}
    \item \textbf{Sargan-Hansen J-test}:
    \[
    J = \left( \frac{1}{\sqrt{N}} \sum_{i=1}^N \mathbf{z}_i' \hat{\mathbf{u}}_i \right)' \hat{\mathbf{W}} \left( \frac{1}{\sqrt{N}} \sum_{i=1}^N \mathbf{z}_i' \hat{\mathbf{u}}_i \right) \stackrel{d}{\to} \chi^2_{q-k},
    \]
    where $q$ is number of instruments, $k$ number of parameters.
    \item Tests validity of moment conditions (instruments).
    \item Difference-in-Sargan test: Compare subsets of instruments.
\end{itemize}

\textbf{3. Hausman Tests}:
\begin{itemize}
    \item Compare fixed effects vs. random effects estimators:
    \[
    H = (\hat{\boldsymbol{\beta}}_{\text{FE}} - \hat{\boldsymbol{\beta}}_{\text{RE}})' [\widehat{\text{Var}}(\hat{\boldsymbol{\beta}}_{\text{FE}}) - \widehat{\text{Var}}(\hat{\boldsymbol{\beta}}_{\text{RE}})]^{-1} (\hat{\boldsymbol{\beta}}_{\text{FE}} - \hat{\boldsymbol{\beta}}_{\text{RE}}) \sim \chi^2_k.
    \]
    \item For dynamic models, compare GMM estimators with different instrument sets.
\end{itemize}

\textbf{4. Unit Root Tests for Panel Data}:
\begin{itemize}
    \item \textbf{Levin-Lin-Chu (LLC) test}: Assumes common unit root process.
    \item \textbf{Im-Pesaran-Shin (IPS) test}: Allows for individual unit root processes.
    \item \textbf{Maddala-Wu Fisher test}: Combines p-values from individual ADF tests.
\end{itemize}

\subsubsection{Inference on Parameters}

\textbf{Wald tests} for linear restrictions $H_0: \mathbf{R}\boldsymbol{\theta} = \mathbf{r}$:
\[
W = (\mathbf{R}\hat{\boldsymbol{\theta}} - \mathbf{r})' [\mathbf{R} \widehat{\text{Var}}(\hat{\boldsymbol{\theta}}) \mathbf{R}']^{-1} (\mathbf{R}\hat{\boldsymbol{\theta}} - \mathbf{r}) \sim \chi^2_q,
\]
where $q$ is number of restrictions.

\noindent For GMM, use robust variance estimator:
\[
\widehat{\text{Var}}(\hat{\boldsymbol{\theta}}_{\text{GMM}}) = (\mathbf{X}'\mathbf{Z} \mathbf{W}_N \mathbf{Z}'\mathbf{X})^{-1} \mathbf{X}'\mathbf{Z} \mathbf{W}_N \hat{\mathbf{S}} \mathbf{W}_N \mathbf{Z}'\mathbf{X} (\mathbf{X}'\mathbf{Z} \mathbf{W}_N \mathbf{Z}'\mathbf{X})^{-1},
\]
where $\hat{\mathbf{S}}$ estimates $\text{Var}(N^{-1/2}\sum \mathbf{z}_i' \mathbf{u}_i)$.

\subsection{Practical Considerations}

\begin{enumerate}
    \item \textbf{Instrument proliferation}: Too many instruments in GMM can overfit endogenous variables and bias results. Use:
    \begin{itemize}
        \item Collapsing instruments (combining lags)
        \item Limiting lag depth
        \item Principal components of instruments
    \end{itemize}
    
    \item \textbf{Weak instruments}: When lagged levels are poor predictors of differences (high persistence). Check with:
    \begin{itemize}
        \item First-stage F-statistics
        \item Stock-Yogo critical values
        \item Anderson-Rubin tests
    \end{itemize}
    
    \item \textbf{Choice between difference and system GMM}:
    \begin{itemize}
        \item Use difference GMM when series are stationary
        \item Use system GMM for persistent data or when $\alpha$ is near 1
        \item Compare with Hansen test for additional moments
    \end{itemize}
    
    \item \textbf{Time effects}: Include time dummies to account for common shocks:
    \[
    y_{it} = \alpha y_{i,t-1} + \mathbf{x}_{it}'\boldsymbol{\beta} + \eta_i + \delta_t + \varepsilon_{it}.
    \]
    
    \item \textbf{Nonlinear dynamic panels}: For binary, count, or other nonlinear outcomes:
    \begin{itemize}
        \item Dynamic probit/logit with correlated random effects
        \item Conditional maximum likelihood for logit
        \item GMM for exponential regression models
    \end{itemize}
\end{enumerate}

\subsection{Software Implementation}

\textbf{Stata}:
\begin{verbatim}
* Arellano-Bond difference GMM
xtabond y x1 x2, lags(1)
estat abond  // Test autocorrelation
estat sargan // Test overidentifying restrictions

* Blundell-Bond system GMM
xtdpdsys y x1 x2, lags(1)
\end{verbatim}

\textbf{R}:
\begin{verbatim}
library(plm)
# Difference GMM
gmm_diff <- pgmm(y ~ lag(y, 1) + x1 + x2 | lag(y, 2:99),
                 data = pdata, effect = "individual", 
                 model = "twosteps")
summary(gmm_diff)

# System GMM  
gmm_sys <- pgmm(y ~ lag(y, 1) + x1 + x2 | lag(y, 2:99),
                data = pdata, effect = "individual",
                model = "onestep", transformation = "ld")
summary(gmm_sys)
\end{verbatim}

\subsection{Summary}

Time-dependent panel models capture important dynamic relationships but require careful estimation due to:
\begin{itemize}
    \item The inherent endogeneity from lagged dependent variables
    \item Unobserved heterogeneity correlated with regressors
    \item Potential serial correlation in errors
\end{itemize}

GMM methods, particularly Arellano-Bond and Blundell-Bond estimators, are standard for short panels. For longer panels, bias-corrected fixed effects or MLE may be appropriate. Comprehensive specification testing is crucial for valid inference
\end{document}